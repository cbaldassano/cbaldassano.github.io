<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Rooting for the machines &middot; A Blog by Chris Baldassano
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">


  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="Blog feed" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15871783-1', 'auto');
  ga('send', 'pageview');

  </script>

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@ChrisBaldassano">
  <meta name="twitter:creator" content="@ChrisBaldassano">
  
    <meta name="twitter:title" content="Home">
  
  
    <meta name="twitter:url" content="http://blog.chrisbaldassano.com//page2/">
  
  
    <meta name="twitter:description" content="A blog about real minds, artificial minds, using artificial minds to understand real minds, and using real minds to inspire artificial minds">
  


</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A blog about real minds, artificial minds, using artificial minds to understand real minds, and using real minds to inspire artificial minds</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://www.chrisbaldassano.com">Main</a>
    <a class="sidebar-nav-item" href="/blog/">Blog</a>


    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/blog/archive/">Blog Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    

    <a class="sidebar-nav-item" href="/blog/atom.xml">Atom feed</a>
    
  </nav>

  <div class="sidebar-item">
    <p>
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/blog" title="Home">Rooting for the machines</a>
            <small>A Blog by Chris Baldassano</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2016/10/25/twonet/">
        Connecting past and present in visual perception
      </a>
    </h1>

    <span class="post-date">25 Oct 2016</span>

    <div class="message">
<p>There are two kinds of people in the world—those who divide everything in the world into two kinds of things and those who don’t.
<p>Kenneth Boulding
</div>

<p>Scientists love dividing the world into categories. Whenever we are trying to study more than 1 or 2 things at a time, our first instinct is to sort them into boxes based on their similarities, whether we&#39;re looking at animals, rocks, stars, or diseases.</p>

<p>There have been many proposals on how to divide up the human visual system: regions processing coarse vs. fine structure, or small objects vs. big objects, or <a href="/blog/2016/05/20/corner/">central vs. peripheral</a> information. In my new paper, <a href="http://eneuro.org/content/3/5/ENEURO.0178-16.2016">Two distinct scene processing networks connecting vision and memory</a>, I argue that regions activated by photographic images can be split into two different networks.
<br/>
<br/></p>

<table><thead>
<tr>
<th style="text-align: center"><img src="/blog/public/visnet.png" alt="Visual network"></th>
<th style="text-align: center"><img src="/blog/public/visnet_ex.png" alt="Visual network example"></th>
</tr>
</thead><tbody>
</tbody></table>

<p>The first group of scene-processing regions (near the back of the brain) care only about the image that is currently coming in through your eyes. They are looking for visual features like walls, landmarks, and architecture that will help you determine the structure of the environment around you. But they don&#39;t try to keep track of this information over time - as soon as you move your eyes, they forget all about the last view of the world.</p>

<p><br/>
<br/></p>

<table><thead>
<tr>
<th style="text-align: center"><img src="/blog/public/memnet.png" alt="Memory navigation network"></th>
<th style="text-align: center"><img src="/blog/public/photosynth_5fps.gif" alt="Memory network example"></th>
</tr>
</thead><tbody>
</tbody></table>

<p>The second group (a bit farther forward) uses the information from the first group to build up a stable model of the world and your place in it. They care less about exactly where your eyes are pointed and more about where you are in the world, creating a 3D model of the room or landscape around you and placing you on a map of what other places are nearby. These regions are strongly linked to your long-term memory system, and show the highest activity in familiar environments.</p>

<p>I am very interested in this second group of regions that integrate information over time - what exactly are they keeping track of, and how do they get information in and out of long-term memory? I have a new manuscript with my collaborators at Princeton (currently working its way through the publication gaunlet) showing that these regions build <a href="http://biorxiv.org/content/early/2016/10/14/081018">abstract representations of events in movies and audio narration</a>, and am  running a new experiment looking at how event templates we learn over our lifetimes are used to help build these event representations.</p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2016/07/08/deep/">
        How deep is the brain?
      </a>
    </h1>

    <span class="post-date">08 Jul 2016</span>

    <p>Recent AI advances in <a href="https://research.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html">speech recognition</a>, <a href="https://deepmind.com/alpha-go">game-playing</a>, <a href="http://www.nytimes.com/2014/11/18/science/researchers-announce-breakthrough-in-content-recognition-software.html">image understanding</a>, and <a href="https://research.googleblog.com/2015/07/how-google-translate-squeezes-deep.html">language translation</a> have all been based on a simple concept: multiply some numbers together, set some of them to zero, and then repeat. Since &quot;multiplying and zeroing&quot; doesn&#39;t inspire investors to start throwing money at you, these models are instead presented under the much loftier banner of &quot;deep neural networks.&quot; Ever since the first versions of these networks were invented by Frank Rosenblatt in 1957, there has been controversy over how &quot;neural&quot; these models are. The New York Times proclaimed these first programs (which could accomplish tasks as astounding as distinguishing shapes on the left side versus shapes on the right side of a paper) to be &quot;the first device to think as the human brain.&quot;</p>

<p><img src="/blog/public/nytimes_crop.png" alt="NYT"></p>

<p>Deep neural networks remained mostly a fringe idea for decades, since they typically didn&#39;t perform very well, due (in retrospect) to the limited computational power and small dataset sizes of the era. But over the past decade these networks have begun to rival human capabilities on highly complicated tasks, making it more plausible that they could really be emulating human brains. We&#39;ve also started to get much better data about how the brain itself operates, so we can start to make some comparisons.</p>

<p>At least for visual images, a consensus started to emerge about what these deep neural networks were actually doing, and how it matched up to the brain. These networks operate as a series of &quot;multiply and zero&quot; filters, which build up more and more complicated descriptions of the image. The first filter looks for lines, the second filter combines the lines into corners and curves, the third filter combines the corners into shapes, etc. If we look in the visual system of the brain, we find a similar layered structure, with the early layers of the brain doing something like the early filters of the neural networks, and later layers of the brain looking like the later filters of the neural networks.</p>

<p><img src="/blog/public/layers.png" alt="NN and brain layers">
Zeiler &amp; Fergus 2014, Güçlü &amp; van Gerven 2015</p>

<p>It seemed like things were mostly making sense, until two recent developments:
1. The best-performing networks started requiring a <em>lot</em> of filters. For example, one of the <a href="http://arxiv.org/abs/1603.05027">current state-of-the-art networks</a> uses 1,001 layers. Although we don&#39;t know exactly how many layers the brain&#39;s visual system has, it is almost certainly less than 100.<br>
2. These networks actually don&#39;t get that much worse if you randomly remove layers from the <em>middle</em> of the chain. This makes very little sense if you think that each filter is combining shapes from the previous filter - it&#39;s like saying that you can skip one step of a recipe and things will still work out fine.</p>

<p>Should we just throw up our hands and say that these networks just have way more layers than the brain (they&#39;re &quot;deeper&quot;) and we can&#39;t understand how they work? <a href="http://arxiv.org/abs/1604.03640">Liao and Poggio</a> have a recent preprint that proposes a possible solution to both of these issues: maybe the later layers are all doing the same operation over and over, so that the filter chain looks like this:</p>

<p><img src="/blog/public/unrolled.png" alt="Feedforward NN"></p>

<p>Why would you want to repeat the same operation many times? Often it is a lot easier to figure out how to make a small step toward your goal and then repeat, instead of going directly to the goal. For example, imagine you want to set a microwave for twelve minutes, but all the buttons are unlabeled and in random positions. Typing 1-2-0-0-GO is going to take a lot of trial and error, and if you mess up in the middle you have to start from scratch. But if you&#39;re able to find the &quot;add 30 seconds&quot; button, you can just hit it 24 times and you&#39;ll be set. This also shows why skipping a step isn&#39;t a big deal - if you hit the button 23 times instead, it shouldn&#39;t cause major issues.</p>

<p>But if the last layers are just the same filter over and over, we can actually just replace them with a single filter in a loop, that takes its output and feeds it back into its input. This will act like a deep network, except that the extra layers are occurring in <em>time</em>:</p>

<p><img src="/blog/public/rnn.png" alt="Recurrent NN"></p>

<p>So Liao and Poggio&#39;s hypothesis is that <strong>very deep neural networks are like a brain that is moderately deep in both space and time.</strong> The true depth of the brain is hidden, since even though it doesn&#39;t have a huge number of regions it gets to run these regions in loops over time. Their paper has some experiments to show that this is plausible, but it will take some careful comparisons with neuroscience data to say if they are correct.</p>

<p>Of course, it seems inevitable that at some point in the near future we will in fact start building neural networks that are &quot;deeper&quot; than the brain, in one way or another. Even if we don&#39;t discover new models that can learn better than a brain can, computers have lots of unfair advantages - they&#39;re not limited to a 1500 cm<sup>3</sup> skull, they have direct access to the internet, they can instantly teach each other things they&#39;ve learned, and they never get bored. Once we have a neural network that is similar in complexity to the human brain but can run on computer hardware, its capabilities might be advanced enough to design an even more intelligent machine on its own, and so on: maybe the &quot;first ultraintelligent machine is the <em>last</em> invention that man need ever make.&quot; <a href="http://mindstalk.net/vinge/vinge-sing.html">(Vernor Vinge)</a> </p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2016/05/20/corner/">
        The corner of your eye
      </a>
    </h1>

    <span class="post-date">20 May 2016</span>

    <p>We usually think that our eyes work like a camera, giving us a sharp, colorful picture of the world all the way from left to right and top to bottom. But we actually only get this kind of detail in a tiny window right where our eyes are pointed. If you hold your thumb out at arm&#39;s length, the width of your thumbnail is about the size of your most precise central (also called &quot;foveal&quot;) vision. Outside of that narrow spotlight, both color perception and sharpness drop off rapidly - doing high-precision tasks like reading a word is almost impossible unless you&#39;re looking right at it.</p>

<p>The rest of your visual field is your &quot;peripheral&quot; vision, which has only imprecise information about shape, location, and color. Out here in the corner of your eye you can&#39;t be sure of much, which is used as a constant source of fear and uncertainty in horror movies and the occult:</p>

<div class="message">
<p>What's that in the mirror, or the corner of your eye?

<p>What's that footstep following, but never passing by?

<p>Perhaps they're all just waiting, perhaps when we're all dead,

<p>Out they'll come a-slithering from underneath the bed.... 
<p><a href="https://www.youtube.com/watch?v=vJiJZiOaJFQ">Doctor Who "Listen"</a>
</div>

<p>What does this peripheral information get used for during visual processing? It was shown over a decade ago (by one of my current mentors, <a href="http://www.weizmann.ac.il/neurobiology/labs/malach/sites/neurobiology.labs.malach/files/2002_04_Hasson_neuron.pdf">Uri Hasson</a>) that flashing pictures in your central and peripheral vision activate different brain regions. The hypothesis is that peripheral information gets used for tasks like determining where you are, learning the layout of the room around you, and planning where to look next. But this experimental setup is pretty unrealistic. In real life we have related information coming into both central and peripheral vision at the same time, which is constantly changing and depends on where we decide to look. Can we track how visual information flows through the brain during natural viewing?</p>

<p>Today a new paper from me and my PhD advisors (<a href="http://vision.stanford.edu/feifeili/">Fei-Fei Li</a> and <a href="http://www.psychology.illinois.edu/people/dmbeck">Diane Beck</a>) is out in the Journal of Vision: <a href="http://jov.arvojournals.org/article.aspx?articleid=2524115">Pinpointing the peripheral bias in neural scene-processing networks during natural viewing (open access)</a>. I looked at fMRI data (collected and shared generously by Mike Arcaro,Sabine Kastner, Janice Chen, and Asieh Zadbood) while people were watching clips from movies and TV shows. They were free to move their eyes around and watch as you normally would, except that they were inside a huge superconducting magnet rather than on the couch (and had less popcorn). We can disentangle central and peripheral information by tracking how these streams flow out of their initial processing centers in visual cortex to regions performing more complicated functions like object recognition and navigation.</p>

<p>We can make maps that show where foveal information ends up (colored orange/red) and where peripheral information ends up (colored blue/purple). I&#39;m showing this on an &quot;inflated&quot; brain surface where we&#39;ve smoothed out all the wrinkles to make it easier to look at:</p>

<p><img src="/blog/public/periphery.png" alt="Foveal and Peripheral regions"></p>

<p>This roughly matches what we had previously seen with the simpler experiments: central information heads to regions for recognizing objects, letters, and faces, while peripheral information gets used by areas that process environments and big landmarks. But it also reveals some finer structure we didn&#39;t know about before. Some scene processing regions care more about the &quot;near&quot; periphery just outside the fovea and still have access to relatively high-resolution information, while others draw information from the &quot;far&quot; periphery that only provides coarse information about your current location. There are also detectable foveal vs. peripheral differences in the frontal lobe of the brain, which is pretty surprising, since this part of the brain is supposed to be performing abstract reasoning and planning that shouldn&#39;t be all that related to where the information is coming from.</p>

<p>This paper was my first foray into the fun world of movie-watching data, which I&#39;ve become obsessed with during my postdoc. Contrary to the what everyone&#39;s parents told them, watching TV doesn&#39;t turn off your brain - you use almost every part of your brain to understand and follow along with the story, and answering questions about videos is such a challenging problem that even the latest computer AIs are pretty terrible at it (though some of <a href="http://vision.stanford.edu/publications.html">my former labmates</a> have started making them better). We&#39;re finding that movies drive much stronger and more complex activity patterns compared to the usual paradigm of flashing individual images, and we&#39;re starting to answer questions raised by cognitive scientists in the 1970s about how complicated situations are understood and remembered - stay tuned!</p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2016/04/13/configural/">
        Cutups and configural processing
      </a>
    </h1>

    <span class="post-date">13 Apr 2016</span>

    <div class="message">
“The love of complexity without reductionism makes art; the love of complexity with reductionism makes science.” — E.O. Wilson
</div>

<p>In the 1950s William S. Burroughs popularized an art form called the &quot;cut-up technique.&quot; The idea was to take existing stories (in text, audio, or video) and cut them up into pieces, and then recombine them into something new. His creations are a juxaposition of (often disturbing) imagery, chosen to fit together despite coming from different sources. Here&#39;s a sample from <em>The Soft Machine</em>:</p>

<div class="message">
Police files of the world spurt out in a blast of bone meal, garden tools and barbecue sets whistle through the air, skewer the spectators - crumpled cloth bodies through dead nitrous streets of an old film set - grey luminous flakes falling softly on Ewyork, Onolulu, Aris, Ome, Osteon - From siren towers the twanging notes of fear - Pan God of Panic piping blue notes through empty streets as the berserk time machine twisted a tornado of years and centuries-
</div>

<p>The cut-ups aren&#39;t always coherent in the sense of having an understandable plot - sometimes Burroughs was just aiming to convey an emotion. He attributed an almost mystical quality to cut-ups, saying they could help reveal the hidden meanings in text or even serve as prophecy, since &quot;when you cut into the present the future leaks out.&quot; His experimental film <em>The Cut-Ups</em> was predictably polarizing, with some people finding it mesmerizing and others demanding their money back.</p>

<iframe  title="The Cut-Ups" width="480" height="390" src="http://www.youtube.com/embed/Uq_hztHJCM4" frameborder="0"></iframe>

<p>If you jump through the video a bit you&#39;ll see that it isn&#39;t quite as repetitive as it seems during the first minute. (I also think Burroughs would heartily approve of jumping through the movie rather than watching it from beginning to end.)</p>

<p>This idea of combining parts to create something new is alive and well on the internet, especially now that we are starting to amass a huge library of video and audio clips. It&#39;s painstaking work, but there is a whole genre of videos in which clips from public figures are put together to <a href="https://www.youtube.com/watch?v=7CYJ73pVpVc">recreate</a> or <a href="https://www.youtube.com/watch?v=A7vTkbnjJcQ">parody</a> existing songs, or to create <a href="https://www.youtube.com/watch?v=Gg5SwyTvAHw">totally original</a> compositions.</p>

<p>Since the whole can have a meaning that is more than the sum of its parts, our brains must be somehow putting these parts together. This process is referred to as &quot;configural processing,&quot; since understanding what we&#39;re hearing or seeing requires looking not just at the parts but at their configuration. Work from <a href="http://media.wix.com/ugd/b75639_179a3f18a5774393b1bf37e9c1d1cb11.pdf">Uri Hasson&#39;s lab</a> (before I joined as a postdoc) has looked at how meaning gets pieced together throughout a story, and found a network of brain regions that help join sentences together to understand a narrative. They used stimuli very similar to the cut-ups, in which sentences were cut out and then put back together in a random order, and showed that these brain regions stopped responding consistently when the overall meaning was taken away (even though the parts were the same).</p>

<p>Today I (along with my PhD advisors, <a href="http://vision.stanford.edu/feifeili/">Fei-Fei Li</a> and <a href="http://www.psychology.illinois.edu/people/dmbeck">Diane Beck</a>) have a new paper out in Cerebral Cortex, titled <a href="http://cercor.oxfordjournals.org/cgi/reprint/bhw077?ijkey=iUBNzaBCkEO1hN4&keytype=ref">Human-object interactions are more than the sum of their parts (free-access link)</a>. This paper looks at how things get combined across space (rather than time) in the visual system. We were looking specifically at images containing either a person, an object, or both, and tried to find brain regions where a <em>meaningful</em> human-object interaction looked different from just a sum of person plus object.</p>

<p><img src="/blog/public/sum_of_parts.png" alt="SumOfParts"></p>

<p>In the full paper we look at a number of different brain regions, but some of the most interesting results come from the superior temporal sulcus (an area right behind the top of your ears). This area couldn&#39;t care less about objects by themselves, and doesn&#39;t even care much about people if they aren&#39;t doing anything. But as soon as we put the person and object together in a meaningful way, it starts paying attention, and we can make a better-than-chance guess about what action the person is performing (in the picture you&#39;re currently looking at) just by reading your brain activity from this region. Our current theory about this region is that it is involved in understanding the actions and intentions of other people, as I described in <a href="/blog/2015/02/24/conchords/">a previous post</a>.</p>

<p>Next month I&#39;ll be presenting at <a href="http://memory.psych.upenn.edu/CEMS_2016">CEMS 2016</a> on some new work I&#39;ve been doing with Uri and Ken Norman, where I&#39;m trying to figure out exactly which pieces of a story end up getting combined together and how these combined representations get stored into memory. Working with real stories (like movies and TV shows) is challenging as a scientist, since usually we like our stimuli to be very tightly controlled, but these kinds of creative, meaningful stimuli can give us a window into the most interesting functions of the brain.</p>

<div class="message">
<p>Interviewer:  In view of all this, what will happen to fiction in the next twenty-five years?

<p>Burroughs: In the first place, I think there's going to be more and more merging of art and science. Scientists are already studying the creative process, and I think the whole line between art and science will break down and that scientists, I hope, will become more creative and writers more scientific. [...] Science will also discover for us how association blocks actually form.

<p>Interviewer: Do you think this will destroy the magic? 

<p>Burroughs: Not at all. I would say it would enhance it.

<p><a href="http://www.theparisreview.org/interviews/4424/the-art-of-fiction-no-36-william-s-burroughs">William S. Burroughs, The Art of Fiction No. 36</a>
</div>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2015/12/01/race/">
        Just how Amazing is The Amazing Race?
      </a>
    </h1>

    <span class="post-date">01 Dec 2015</span>

    <h2>The Amazing Race</h2>

<p>The <a href="https://en.wikipedia.org/wiki/The_Amazing_Race">Amazing Race</a> is one of the few reality TV shows that managed to survive the bubble of the early 2000s, with good reason. Rather than just trying to play up interpersonal dramas (though there is some of that too), it is set up like a traditional game show with a series of competitions between teams of two, who travel to different cities throughout the world over the course of the show. Eleven teams start out the race, and typically the last team to finish each day&#39;s challenges gets booted from the show until only three teams are left. These three teams then have a final day of competition, with the winner being awarded $1 million.</p>

<p>Winning first place on any day before the last one doesn&#39;t matter much (though you get a small prize and some bragging rights), which is interesting, since it means that it is possible for the winning team to have never come in first place before the final leg. This got me wondering: if we think of the Race as an experiment which is trying to identify the best team, how good is it? What if we just gave teams a point for every first place win, and then saw which one got the most points, like a baseball series?</p>

<h2>Modeling the Race</h2>

<p>To try to answer this question, I build a simple model of the Race. I assume that each team has some fixed skill level (sampled from a standard normal distribution), and then on each leg their performance is the sum of this instrinc skill and some randomness (sampled from another normal with varying width). So every leg, the ranking of the teams will be their true skill ranking, plus some randomness (and there can be a lot of randomness on the race). Fans of the show will know that this is a very simplified model of the race (the legs aren&#39;t totally independent, the teams can strategically interfere with each other, etc.) but this captures the basic idea. I ran simulated races 10,000 times for each level of randomness.</p>

<p>We can measure how effective the Race was at picking a winner, by seeing what true skill rank the winning team had. So if the team with the highest skill (number 1) wins, that means the race did a good job. If a team with a low skill rank (like 10) wins, then the race did a very bad job of picking the million-dollar winner. This plot shows the rank of the winning team, compared to chance ((1+11)/2=6).</p>

<p><img src="/blog/public/race/elim_rank.png" alt="Winning rank using loser-elimination"></p>

<p>This actually looks surprisingly good! Even at with lots of leg randomness (more than the actual skill difference between the teams) a team with a relatively high rank tends to win. Once the randomness gets to be an order of magnitude bigger than the differences between teams, the winner starts getting close to random.</p>

<h2>Improving the Race</h2>

<p>But how good is this relative to a simpler kind of competition, where the winner is the team with the most first-place wins? Rather than eliminating teams, all teams race all 9 legs, and the team coming in first the most wins the prize (ties are broken based on which team won most recently). Would this do better or worse?</p>

<p><img src="/blog/public/race/elim_first_rank.png" alt="Winning rank using loser-elimination or first place wins"></p>

<p>Turns out this is a little bit better! In general the rank of the winning team tends to be higher, meaning that a &quot;more deserving&quot; team won the money. But the size of the gap depends on how much randomness there is in each leg of the race. Which point along these curves corresponds to the actual TV show?</p>

<p>To answer this, I took the per-leg rankings from the <a href="http://amazingrace.wikia.com/wiki/Main_Page">Amazing Race Wikia</a> from the past 10 seasons. Yes, there are people way more obsessed with this show than me, who have been together databases of stats from each season. I measured how consistent the rankings were from each leg of the race. If there wasn&#39;t any randomness, we&#39;d expect these to have a perfect (Kendall) correlation, while if each leg is just craziness for all teams then the correlation should be near zero. I found that this correlation varied a bit across seasons, but had a mean of 0.0992. Comparing this to the same calculation from the model, this corresponds to a noise level of about sigma=2.2.</p>

<p><img src="/blog/public/race/tau_rank.png" alt="Leg ranking correlations for each noise level"></p>

<p>At this level of randomness, there is about a 10% advantage for counting-first-places competition: 37.4% of the time it picks a better team to win the money, while 28.5% of the time the current elimination setup picks a better team (they pick the same team 34.1% of the time).</p>

<p><img src="/blog/public/race/frac_compare.png" alt="Comparison of methods at sigma=2.2"></p>

<p>Of course there are some disadvantages to counting first place wins: the requires all teams to run all legs (which is logistically difficult and means we get to know each team less) and the winner might be locked-in before the final leg (ruining the suspense of the grand finale they usually have set up for the final tasks). This is likely a general tradeoff in games like this, between being fair (making the right team more likely to win) and being exciting (keeping the winner more uncertain until the end). As a game show, The Amazing Race probably makes the right choice (entertainment over fairness) but for more serious matters (political debate performance?) maybe we should pay attention to the winner of each round rather than the loser.</p>

<p>All the MATLAB code and ranking data is available on <a href="https://bitbucket.org/cbaldassano/amazing-race">my bitbucket</a>.</p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page3">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog">Newer</a>
    
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
