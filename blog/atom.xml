<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Rooting for the machines</title>
 <link href="http://blog.chrisbaldassano.com//atom.xml" rel="self"/>
 <link href="http://blog.chrisbaldassano.com//"/>
 <updated>2017-06-21T15:38:15-04:00</updated>
 <id>http://blog.chrisbaldassano.com/</id>
 <author>
   <name>Chris Baldassano</name>
   <email></email>
 </author>

 
 <entry>
   <title>Parenting the last human generation</title>
   <link href="http://blog.chrisbaldassano.com//2017/06/05/parent/"/>
   <updated>2017-06-05T00:00:00-04:00</updated>
   <id>http://blog.chrisbaldassano.com//2017/06/05/parent</id>
   <content type="html">&lt;p&gt;For most of human history, parents had a pretty good idea of the kind of world they were preparing their children for. Children would be trained to take over their parents&amp;#39; business, or apprentice in a local trade, or aim for a high-status marriage. Even once children began to have more choice in their futures, it was easy to predict what kind of skills they would need to succeed: reading and handwriting, arithmetic, basic knowledge of science and history.&lt;/p&gt;

&lt;p&gt;As technological progress has accelerated, this predictability is starting to break down. Companies like internet search engines didn&amp;#39;t even exist when most of Google&amp;#39;s 70,000 employees were born, and there is no way their parents could have guessed the kind of work they would eventually be doing. Some of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Skrillex&quot;&gt;best-known musicians in the world&lt;/a&gt; construct songs using software, and don&amp;#39;t play any of the instruments that would have been offered to them in elementary school.&lt;/p&gt;

&lt;p&gt;Given this uncertainty, what kinds of skills and interests should I encourage for my own children? Praticing handwriting, as I spent hours doing in school, would almost certainly be a waste. Same goes for mental math beyond small numbers or estimation, now that everyone carries a caculator. Given how computers are slowly seeping into every object in our house, programming seems like a safe answer, until you hear that researchers are currently building systems that can &lt;a href=&quot;https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html&quot;&gt;design themselves&lt;/a&gt; based on training examples.&lt;/p&gt;

&lt;p&gt;Maybe in a couple decades, being creative and artistic will be more important than having STEM skills. Artificial intelligence is still pretty &lt;a href=&quot;https://arstechnica.com/the-multiverse/2016/06/an-ai-wrote-this-movie-and-its-strangely-moving/&quot;&gt;laughably bad&lt;/a&gt; at writing stories, and AI-based &lt;a href=&quot;http://genekogan.com/works/style-transfer/&quot;&gt;art tools&lt;/a&gt; still require a human at the helm. Even if that changes by the time my kids are starting their careers, there could still be a market for &amp;quot;artisan,&amp;quot; human-made art. Having good emotional intelligence also seems like it will always be helpful, in any world where we have to live with others and with ourselves.&lt;/p&gt;

&lt;p&gt;As confusing as this is for me, it will be immensely harder for my children to be parents. I think of this current generation of toddlers as the last human generation - not because humanity is likely to wipe itself out within the next 20 years (though things are looking &lt;a href=&quot;https://www.washingtonpost.com/news/politics/wp/2017/06/01/all-the-reasons-that-trumps-withdrawal-from-the-paris-climate-agreement-doesnt-make-sense/?utm_term=.f76783e873a8&quot;&gt;increasingly worrying&lt;/a&gt; on that front), but because I expect that by then humans and technology will start to become inseparable. Even now, being separated from our cell phones feels disconcerting - we have offloaded so much of our thinking, memory, and conversations to our devices that we feel smaller without them. By the time my grandchildren are teenagers, I expect that being denied access to technology will be absolutely crippling, to the point that they no longer have a coherent identity &lt;a href=&quot;http://nautil.us/issue/28/2050/dont-worry-smart-machines-will-take-us-with-them&quot;&gt;as a human alone&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When a software update could potentially make any skill obsolete, what skills should we cultivate?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Connecting past and present in visual perception</title>
   <link href="http://blog.chrisbaldassano.com//2016/10/25/twonet/"/>
   <updated>2016-10-25T00:00:00-04:00</updated>
   <id>http://blog.chrisbaldassano.com//2016/10/25/twonet</id>
   <content type="html">&lt;div class=&quot;message&quot;&gt;
&lt;p&gt;There are two kinds of people in the world—those who divide everything in the world into two kinds of things and those who don’t.
&lt;p&gt;Kenneth Boulding
&lt;/div&gt;

&lt;p&gt;Scientists love dividing the world into categories. Whenever we are trying to study more than 1 or 2 things at a time, our first instinct is to sort them into boxes based on their similarities, whether we&amp;#39;re looking at animals, rocks, stars, or diseases.&lt;/p&gt;

&lt;p&gt;There have been many proposals on how to divide up the human visual system: regions processing coarse vs. fine structure, or small objects vs. big objects, or &lt;a href=&quot;/blog/2016/05/20/corner/&quot;&gt;central vs. peripheral&lt;/a&gt; information. In my new paper, &lt;a href=&quot;http://eneuro.org/content/3/5/ENEURO.0178-16.2016&quot;&gt;Two distinct scene processing networks connecting vision and memory&lt;/a&gt;, I argue that regions activated by photographic images can be split into two different networks.
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/public/visnet.png&quot; alt=&quot;Visual network&quot;&gt;&lt;/th&gt;
&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/public/visnet_ex.png&quot; alt=&quot;Visual network example&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;The first group of scene-processing regions (near the back of the brain) care only about the image that is currently coming in through your eyes. They are looking for visual features like walls, landmarks, and architecture that will help you determine the structure of the environment around you. But they don&amp;#39;t try to keep track of this information over time - as soon as you move your eyes, they forget all about the last view of the world.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/public/memnet.png&quot; alt=&quot;Memory navigation network&quot;&gt;&lt;/th&gt;
&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/public/photosynth_5fps.gif&quot; alt=&quot;Memory network example&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;The second group (a bit farther forward) uses the information from the first group to build up a stable model of the world and your place in it. They care less about exactly where your eyes are pointed and more about where you are in the world, creating a 3D model of the room or landscape around you and placing you on a map of what other places are nearby. These regions are strongly linked to your long-term memory system, and show the highest activity in familiar environments.&lt;/p&gt;

&lt;p&gt;I am very interested in this second group of regions that integrate information over time - what exactly are they keeping track of, and how do they get information in and out of long-term memory? I have a new manuscript with my collaborators at Princeton (currently working its way through the publication gaunlet) showing that these regions build &lt;a href=&quot;http://biorxiv.org/content/early/2016/10/14/081018&quot;&gt;abstract representations of events in movies and audio narration&lt;/a&gt;, and am  running a new experiment looking at how event templates we learn over our lifetimes are used to help build these event representations.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How deep is the brain?</title>
   <link href="http://blog.chrisbaldassano.com//2016/07/08/deep/"/>
   <updated>2016-07-08T00:00:00-04:00</updated>
   <id>http://blog.chrisbaldassano.com//2016/07/08/deep</id>
   <content type="html">&lt;p&gt;Recent AI advances in &lt;a href=&quot;https://research.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html&quot;&gt;speech recognition&lt;/a&gt;, &lt;a href=&quot;https://deepmind.com/alpha-go&quot;&gt;game-playing&lt;/a&gt;, &lt;a href=&quot;http://www.nytimes.com/2014/11/18/science/researchers-announce-breakthrough-in-content-recognition-software.html&quot;&gt;image understanding&lt;/a&gt;, and &lt;a href=&quot;https://research.googleblog.com/2015/07/how-google-translate-squeezes-deep.html&quot;&gt;language translation&lt;/a&gt; have all been based on a simple concept: multiply some numbers together, set some of them to zero, and then repeat. Since &amp;quot;multiplying and zeroing&amp;quot; doesn&amp;#39;t inspire investors to start throwing money at you, these models are instead presented under the much loftier banner of &amp;quot;deep neural networks.&amp;quot; Ever since the first versions of these networks were invented by Frank Rosenblatt in 1957, there has been controversy over how &amp;quot;neural&amp;quot; these models are. The New York Times proclaimed these first programs (which could accomplish tasks as astounding as distinguishing shapes on the left side versus shapes on the right side of a paper) to be &amp;quot;the first device to think as the human brain.&amp;quot;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/nytimes_crop.png&quot; alt=&quot;NYT&quot;&gt;&lt;/p&gt;

&lt;p&gt;Deep neural networks remained mostly a fringe idea for decades, since they typically didn&amp;#39;t perform very well, due (in retrospect) to the limited computational power and small dataset sizes of the era. But over the past decade these networks have begun to rival human capabilities on highly complicated tasks, making it more plausible that they could really be emulating human brains. We&amp;#39;ve also started to get much better data about how the brain itself operates, so we can start to make some comparisons.&lt;/p&gt;

&lt;p&gt;At least for visual images, a consensus started to emerge about what these deep neural networks were actually doing, and how it matched up to the brain. These networks operate as a series of &amp;quot;multiply and zero&amp;quot; filters, which build up more and more complicated descriptions of the image. The first filter looks for lines, the second filter combines the lines into corners and curves, the third filter combines the corners into shapes, etc. If we look in the visual system of the brain, we find a similar layered structure, with the early layers of the brain doing something like the early filters of the neural networks, and later layers of the brain looking like the later filters of the neural networks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/layers.png&quot; alt=&quot;NN and brain layers&quot;&gt;
Zeiler &amp;amp; Fergus 2014, Güçlü &amp;amp; van Gerven 2015&lt;/p&gt;

&lt;p&gt;It seemed like things were mostly making sense, until two recent developments:
1. The best-performing networks started requiring a &lt;em&gt;lot&lt;/em&gt; of filters. For example, one of the &lt;a href=&quot;http://arxiv.org/abs/1603.05027&quot;&gt;current state-of-the-art networks&lt;/a&gt; uses 1,001 layers. Although we don&amp;#39;t know exactly how many layers the brain&amp;#39;s visual system has, it is almost certainly less than 100.&lt;br&gt;
2. These networks actually don&amp;#39;t get that much worse if you randomly remove layers from the &lt;em&gt;middle&lt;/em&gt; of the chain. This makes very little sense if you think that each filter is combining shapes from the previous filter - it&amp;#39;s like saying that you can skip one step of a recipe and things will still work out fine.&lt;/p&gt;

&lt;p&gt;Should we just throw up our hands and say that these networks just have way more layers than the brain (they&amp;#39;re &amp;quot;deeper&amp;quot;) and we can&amp;#39;t understand how they work? &lt;a href=&quot;http://arxiv.org/abs/1604.03640&quot;&gt;Liao and Poggio&lt;/a&gt; have a recent preprint that proposes a possible solution to both of these issues: maybe the later layers are all doing the same operation over and over, so that the filter chain looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/unrolled.png&quot; alt=&quot;Feedforward NN&quot;&gt;&lt;/p&gt;

&lt;p&gt;Why would you want to repeat the same operation many times? Often it is a lot easier to figure out how to make a small step toward your goal and then repeat, instead of going directly to the goal. For example, imagine you want to set a microwave for twelve minutes, but all the buttons are unlabeled and in random positions. Typing 1-2-0-0-GO is going to take a lot of trial and error, and if you mess up in the middle you have to start from scratch. But if you&amp;#39;re able to find the &amp;quot;add 30 seconds&amp;quot; button, you can just hit it 24 times and you&amp;#39;ll be set. This also shows why skipping a step isn&amp;#39;t a big deal - if you hit the button 23 times instead, it shouldn&amp;#39;t cause major issues.&lt;/p&gt;

&lt;p&gt;But if the last layers are just the same filter over and over, we can actually just replace them with a single filter in a loop, that takes its output and feeds it back into its input. This will act like a deep network, except that the extra layers are occurring in &lt;em&gt;time&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/rnn.png&quot; alt=&quot;Recurrent NN&quot;&gt;&lt;/p&gt;

&lt;p&gt;So Liao and Poggio&amp;#39;s hypothesis is that &lt;strong&gt;very deep neural networks are like a brain that is moderately deep in both space and time.&lt;/strong&gt; The true depth of the brain is hidden, since even though it doesn&amp;#39;t have a huge number of regions it gets to run these regions in loops over time. Their paper has some experiments to show that this is plausible, but it will take some careful comparisons with neuroscience data to say if they are correct.&lt;/p&gt;

&lt;p&gt;Of course, it seems inevitable that at some point in the near future we will in fact start building neural networks that are &amp;quot;deeper&amp;quot; than the brain, in one way or another. Even if we don&amp;#39;t discover new models that can learn better than a brain can, computers have lots of unfair advantages - they&amp;#39;re not limited to a 1500 cm&lt;sup&gt;3&lt;/sup&gt; skull, they have direct access to the internet, they can instantly teach each other things they&amp;#39;ve learned, and they never get bored. Once we have a neural network that is similar in complexity to the human brain but can run on computer hardware, its capabilities might be advanced enough to design an even more intelligent machine on its own, and so on: maybe the &amp;quot;first ultraintelligent machine is the &lt;em&gt;last&lt;/em&gt; invention that man need ever make.&amp;quot; &lt;a href=&quot;http://mindstalk.net/vinge/vinge-sing.html&quot;&gt;(Vernor Vinge)&lt;/a&gt; &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The corner of your eye</title>
   <link href="http://blog.chrisbaldassano.com//2016/05/20/corner/"/>
   <updated>2016-05-20T00:00:00-04:00</updated>
   <id>http://blog.chrisbaldassano.com//2016/05/20/corner</id>
   <content type="html">&lt;p&gt;We usually think that our eyes work like a camera, giving us a sharp, colorful picture of the world all the way from left to right and top to bottom. But we actually only get this kind of detail in a tiny window right where our eyes are pointed. If you hold your thumb out at arm&amp;#39;s length, the width of your thumbnail is about the size of your most precise central (also called &amp;quot;foveal&amp;quot;) vision. Outside of that narrow spotlight, both color perception and sharpness drop off rapidly - doing high-precision tasks like reading a word is almost impossible unless you&amp;#39;re looking right at it.&lt;/p&gt;

&lt;p&gt;The rest of your visual field is your &amp;quot;peripheral&amp;quot; vision, which has only imprecise information about shape, location, and color. Out here in the corner of your eye you can&amp;#39;t be sure of much, which is used as a constant source of fear and uncertainty in horror movies and the occult:&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
&lt;p&gt;What's that in the mirror, or the corner of your eye?

&lt;p&gt;What's that footstep following, but never passing by?

&lt;p&gt;Perhaps they're all just waiting, perhaps when we're all dead,

&lt;p&gt;Out they'll come a-slithering from underneath the bed.... 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vJiJZiOaJFQ&quot;&gt;Doctor Who &quot;Listen&quot;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;What does this peripheral information get used for during visual processing? It was shown over a decade ago (by one of my current mentors, &lt;a href=&quot;http://www.weizmann.ac.il/neurobiology/labs/malach/sites/neurobiology.labs.malach/files/2002_04_Hasson_neuron.pdf&quot;&gt;Uri Hasson&lt;/a&gt;) that flashing pictures in your central and peripheral vision activate different brain regions. The hypothesis is that peripheral information gets used for tasks like determining where you are, learning the layout of the room around you, and planning where to look next. But this experimental setup is pretty unrealistic. In real life we have related information coming into both central and peripheral vision at the same time, which is constantly changing and depends on where we decide to look. Can we track how visual information flows through the brain during natural viewing?&lt;/p&gt;

&lt;p&gt;Today a new paper from me and my PhD advisors (&lt;a href=&quot;http://vision.stanford.edu/feifeili/&quot;&gt;Fei-Fei Li&lt;/a&gt; and &lt;a href=&quot;http://www.psychology.illinois.edu/people/dmbeck&quot;&gt;Diane Beck&lt;/a&gt;) is out in the Journal of Vision: &lt;a href=&quot;http://jov.arvojournals.org/article.aspx?articleid=2524115&quot;&gt;Pinpointing the peripheral bias in neural scene-processing networks during natural viewing (open access)&lt;/a&gt;. I looked at fMRI data (collected and shared generously by Mike Arcaro,Sabine Kastner, Janice Chen, and Asieh Zadbood) while people were watching clips from movies and TV shows. They were free to move their eyes around and watch as you normally would, except that they were inside a huge superconducting magnet rather than on the couch (and had less popcorn). We can disentangle central and peripheral information by tracking how these streams flow out of their initial processing centers in visual cortex to regions performing more complicated functions like object recognition and navigation.&lt;/p&gt;

&lt;p&gt;We can make maps that show where foveal information ends up (colored orange/red) and where peripheral information ends up (colored blue/purple). I&amp;#39;m showing this on an &amp;quot;inflated&amp;quot; brain surface where we&amp;#39;ve smoothed out all the wrinkles to make it easier to look at:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/periphery.png&quot; alt=&quot;Foveal and Peripheral regions&quot;&gt;&lt;/p&gt;

&lt;p&gt;This roughly matches what we had previously seen with the simpler experiments: central information heads to regions for recognizing objects, letters, and faces, while peripheral information gets used by areas that process environments and big landmarks. But it also reveals some finer structure we didn&amp;#39;t know about before. Some scene processing regions care more about the &amp;quot;near&amp;quot; periphery just outside the fovea and still have access to relatively high-resolution information, while others draw information from the &amp;quot;far&amp;quot; periphery that only provides coarse information about your current location. There are also detectable foveal vs. peripheral differences in the frontal lobe of the brain, which is pretty surprising, since this part of the brain is supposed to be performing abstract reasoning and planning that shouldn&amp;#39;t be all that related to where the information is coming from.&lt;/p&gt;

&lt;p&gt;This paper was my first foray into the fun world of movie-watching data, which I&amp;#39;ve become obsessed with during my postdoc. Contrary to the what everyone&amp;#39;s parents told them, watching TV doesn&amp;#39;t turn off your brain - you use almost every part of your brain to understand and follow along with the story, and answering questions about videos is such a challenging problem that even the latest computer AIs are pretty terrible at it (though some of &lt;a href=&quot;http://vision.stanford.edu/publications.html&quot;&gt;my former labmates&lt;/a&gt; have started making them better). We&amp;#39;re finding that movies drive much stronger and more complex activity patterns compared to the usual paradigm of flashing individual images, and we&amp;#39;re starting to answer questions raised by cognitive scientists in the 1970s about how complicated situations are understood and remembered - stay tuned!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Cutups and configural processing</title>
   <link href="http://blog.chrisbaldassano.com//2016/04/13/configural/"/>
   <updated>2016-04-13T00:00:00-04:00</updated>
   <id>http://blog.chrisbaldassano.com//2016/04/13/configural</id>
   <content type="html">&lt;div class=&quot;message&quot;&gt;
“The love of complexity without reductionism makes art; the love of complexity with reductionism makes science.” — E.O. Wilson
&lt;/div&gt;

&lt;p&gt;In the 1950s William S. Burroughs popularized an art form called the &amp;quot;cut-up technique.&amp;quot; The idea was to take existing stories (in text, audio, or video) and cut them up into pieces, and then recombine them into something new. His creations are a juxaposition of (often disturbing) imagery, chosen to fit together despite coming from different sources. Here&amp;#39;s a sample from &lt;em&gt;The Soft Machine&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
Police files of the world spurt out in a blast of bone meal, garden tools and barbecue sets whistle through the air, skewer the spectators - crumpled cloth bodies through dead nitrous streets of an old film set - grey luminous flakes falling softly on Ewyork, Onolulu, Aris, Ome, Osteon - From siren towers the twanging notes of fear - Pan God of Panic piping blue notes through empty streets as the berserk time machine twisted a tornado of years and centuries-
&lt;/div&gt;

&lt;p&gt;The cut-ups aren&amp;#39;t always coherent in the sense of having an understandable plot - sometimes Burroughs was just aiming to convey an emotion. He attributed an almost mystical quality to cut-ups, saying they could help reveal the hidden meanings in text or even serve as prophecy, since &amp;quot;when you cut into the present the future leaks out.&amp;quot; His experimental film &lt;em&gt;The Cut-Ups&lt;/em&gt; was predictably polarizing, with some people finding it mesmerizing and others demanding their money back.&lt;/p&gt;

&lt;iframe  title=&quot;The Cut-Ups&quot; width=&quot;480&quot; height=&quot;390&quot; src=&quot;http://www.youtube.com/embed/Uq_hztHJCM4&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;If you jump through the video a bit you&amp;#39;ll see that it isn&amp;#39;t quite as repetitive as it seems during the first minute. (I also think Burroughs would heartily approve of jumping through the movie rather than watching it from beginning to end.)&lt;/p&gt;

&lt;p&gt;This idea of combining parts to create something new is alive and well on the internet, especially now that we are starting to amass a huge library of video and audio clips. It&amp;#39;s painstaking work, but there is a whole genre of videos in which clips from public figures are put together to &lt;a href=&quot;https://www.youtube.com/watch?v=7CYJ73pVpVc&quot;&gt;recreate&lt;/a&gt; or &lt;a href=&quot;https://www.youtube.com/watch?v=A7vTkbnjJcQ&quot;&gt;parody&lt;/a&gt; existing songs, or to create &lt;a href=&quot;https://www.youtube.com/watch?v=Gg5SwyTvAHw&quot;&gt;totally original&lt;/a&gt; compositions.&lt;/p&gt;

&lt;p&gt;Since the whole can have a meaning that is more than the sum of its parts, our brains must be somehow putting these parts together. This process is referred to as &amp;quot;configural processing,&amp;quot; since understanding what we&amp;#39;re hearing or seeing requires looking not just at the parts but at their configuration. Work from &lt;a href=&quot;http://media.wix.com/ugd/b75639_179a3f18a5774393b1bf37e9c1d1cb11.pdf&quot;&gt;Uri Hasson&amp;#39;s lab&lt;/a&gt; (before I joined as a postdoc) has looked at how meaning gets pieced together throughout a story, and found a network of brain regions that help join sentences together to understand a narrative. They used stimuli very similar to the cut-ups, in which sentences were cut out and then put back together in a random order, and showed that these brain regions stopped responding consistently when the overall meaning was taken away (even though the parts were the same).&lt;/p&gt;

&lt;p&gt;Today I (along with my PhD advisors, &lt;a href=&quot;http://vision.stanford.edu/feifeili/&quot;&gt;Fei-Fei Li&lt;/a&gt; and &lt;a href=&quot;http://www.psychology.illinois.edu/people/dmbeck&quot;&gt;Diane Beck&lt;/a&gt;) have a new paper out in Cerebral Cortex, titled &lt;a href=&quot;http://cercor.oxfordjournals.org/cgi/reprint/bhw077?ijkey=iUBNzaBCkEO1hN4&amp;amp;keytype=ref&quot;&gt;Human-object interactions are more than the sum of their parts (free-access link)&lt;/a&gt;. This paper looks at how things get combined across space (rather than time) in the visual system. We were looking specifically at images containing either a person, an object, or both, and tried to find brain regions where a &lt;em&gt;meaningful&lt;/em&gt; human-object interaction looked different from just a sum of person plus object.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/sum_of_parts.png&quot; alt=&quot;SumOfParts&quot;&gt;&lt;/p&gt;

&lt;p&gt;In the full paper we look at a number of different brain regions, but some of the most interesting results come from the superior temporal sulcus (an area right behind the top of your ears). This area couldn&amp;#39;t care less about objects by themselves, and doesn&amp;#39;t even care much about people if they aren&amp;#39;t doing anything. But as soon as we put the person and object together in a meaningful way, it starts paying attention, and we can make a better-than-chance guess about what action the person is performing (in the picture you&amp;#39;re currently looking at) just by reading your brain activity from this region. Our current theory about this region is that it is involved in understanding the actions and intentions of other people, as I described in &lt;a href=&quot;/blog/2015/02/24/conchords/&quot;&gt;a previous post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Next month I&amp;#39;ll be presenting at &lt;a href=&quot;http://memory.psych.upenn.edu/CEMS_2016&quot;&gt;CEMS 2016&lt;/a&gt; on some new work I&amp;#39;ve been doing with Uri and Ken Norman, where I&amp;#39;m trying to figure out exactly which pieces of a story end up getting combined together and how these combined representations get stored into memory. Working with real stories (like movies and TV shows) is challenging as a scientist, since usually we like our stimuli to be very tightly controlled, but these kinds of creative, meaningful stimuli can give us a window into the most interesting functions of the brain.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
&lt;p&gt;Interviewer:  In view of all this, what will happen to fiction in the next twenty-five years?

&lt;p&gt;Burroughs: In the first place, I think there's going to be more and more merging of art and science. Scientists are already studying the creative process, and I think the whole line between art and science will break down and that scientists, I hope, will become more creative and writers more scientific. [...] Science will also discover for us how association blocks actually form.

&lt;p&gt;Interviewer: Do you think this will destroy the magic? 

&lt;p&gt;Burroughs: Not at all. I would say it would enhance it.

&lt;p&gt;&lt;a href=&quot;http://www.theparisreview.org/interviews/4424/the-art-of-fiction-no-36-william-s-burroughs&quot;&gt;William S. Burroughs, The Art of Fiction No. 36&lt;/a&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Just how Amazing is The Amazing Race?</title>
   <link href="http://blog.chrisbaldassano.com//2015/12/01/race/"/>
   <updated>2015-12-01T00:00:00-05:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/12/01/race</id>
   <content type="html">&lt;h2&gt;The Amazing Race&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Amazing_Race&quot;&gt;Amazing Race&lt;/a&gt; is one of the few reality TV shows that managed to survive the bubble of the early 2000s, with good reason. Rather than just trying to play up interpersonal dramas (though there is some of that too), it is set up like a traditional game show with a series of competitions between teams of two, who travel to different cities throughout the world over the course of the show. Eleven teams start out the race, and typically the last team to finish each day&amp;#39;s challenges gets booted from the show until only three teams are left. These three teams then have a final day of competition, with the winner being awarded $1 million.&lt;/p&gt;

&lt;p&gt;Winning first place on any day before the last one doesn&amp;#39;t matter much (though you get a small prize and some bragging rights), which is interesting, since it means that it is possible for the winning team to have never come in first place before the final leg. This got me wondering: if we think of the Race as an experiment which is trying to identify the best team, how good is it? What if we just gave teams a point for every first place win, and then saw which one got the most points, like a baseball series?&lt;/p&gt;

&lt;h2&gt;Modeling the Race&lt;/h2&gt;

&lt;p&gt;To try to answer this question, I build a simple model of the Race. I assume that each team has some fixed skill level (sampled from a standard normal distribution), and then on each leg their performance is the sum of this instrinc skill and some randomness (sampled from another normal with varying width). So every leg, the ranking of the teams will be their true skill ranking, plus some randomness (and there can be a lot of randomness on the race). Fans of the show will know that this is a very simplified model of the race (the legs aren&amp;#39;t totally independent, the teams can strategically interfere with each other, etc.) but this captures the basic idea. I ran simulated races 10,000 times for each level of randomness.&lt;/p&gt;

&lt;p&gt;We can measure how effective the Race was at picking a winner, by seeing what true skill rank the winning team had. So if the team with the highest skill (number 1) wins, that means the race did a good job. If a team with a low skill rank (like 10) wins, then the race did a very bad job of picking the million-dollar winner. This plot shows the rank of the winning team, compared to chance ((1+11)/2=6).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/race/elim_rank.png&quot; alt=&quot;Winning rank using loser-elimination&quot;&gt;&lt;/p&gt;

&lt;p&gt;This actually looks surprisingly good! Even at with lots of leg randomness (more than the actual skill difference between the teams) a team with a relatively high rank tends to win. Once the randomness gets to be an order of magnitude bigger than the differences between teams, the winner starts getting close to random.&lt;/p&gt;

&lt;h2&gt;Improving the Race&lt;/h2&gt;

&lt;p&gt;But how good is this relative to a simpler kind of competition, where the winner is the team with the most first-place wins? Rather than eliminating teams, all teams race all 9 legs, and the team coming in first the most wins the prize (ties are broken based on which team won most recently). Would this do better or worse?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/race/elim_first_rank.png&quot; alt=&quot;Winning rank using loser-elimination or first place wins&quot;&gt;&lt;/p&gt;

&lt;p&gt;Turns out this is a little bit better! In general the rank of the winning team tends to be higher, meaning that a &amp;quot;more deserving&amp;quot; team won the money. But the size of the gap depends on how much randomness there is in each leg of the race. Which point along these curves corresponds to the actual TV show?&lt;/p&gt;

&lt;p&gt;To answer this, I took the per-leg rankings from the &lt;a href=&quot;http://amazingrace.wikia.com/wiki/Main_Page&quot;&gt;Amazing Race Wikia&lt;/a&gt; from the past 10 seasons. Yes, there are people way more obsessed with this show than me, who have been together databases of stats from each season. I measured how consistent the rankings were from each leg of the race. If there wasn&amp;#39;t any randomness, we&amp;#39;d expect these to have a perfect (Kendall) correlation, while if each leg is just craziness for all teams then the correlation should be near zero. I found that this correlation varied a bit across seasons, but had a mean of 0.0992. Comparing this to the same calculation from the model, this corresponds to a noise level of about sigma=2.2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/race/tau_rank.png&quot; alt=&quot;Leg ranking correlations for each noise level&quot;&gt;&lt;/p&gt;

&lt;p&gt;At this level of randomness, there is about a 10% advantage for counting-first-places competition: 37.4% of the time it picks a better team to win the money, while 28.5% of the time the current elimination setup picks a better team (they pick the same team 34.1% of the time).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/race/frac_compare.png&quot; alt=&quot;Comparison of methods at sigma=2.2&quot;&gt;&lt;/p&gt;

&lt;p&gt;Of course there are some disadvantages to counting first place wins: the requires all teams to run all legs (which is logistically difficult and means we get to know each team less) and the winner might be locked-in before the final leg (ruining the suspense of the grand finale they usually have set up for the final tasks). This is likely a general tradeoff in games like this, between being fair (making the right team more likely to win) and being exciting (keeping the winner more uncertain until the end). As a game show, The Amazing Race probably makes the right choice (entertainment over fairness) but for more serious matters (political debate performance?) maybe we should pay attention to the winner of each round rather than the loser.&lt;/p&gt;

&lt;p&gt;All the MATLAB code and ranking data is available on &lt;a href=&quot;https://bitbucket.org/cbaldassano/amazing-race&quot;&gt;my bitbucket&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Big Data bust of the 1500s&#58; lessons from the first scientific data miner</title>
   <link href="http://blog.chrisbaldassano.com//2015/05/11/bigdata/"/>
   <updated>2015-05-11T00:00:00-04:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/05/11/bigdata</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/blog/public/geocentric.png&quot; alt=&quot;Geoheliocentric Model&quot;&gt;&lt;/p&gt;

&lt;p&gt;The Big Data craze is in full swing within many scientific fields, especially neuroscience. Since we can&amp;#39;t hope to understand the tangled web of the brain by looking at only one tiny piece, groups have started to amass huge datasets describing brain &lt;a href=&quot;http://neurosynth.org/&quot;&gt;function&lt;/a&gt; and &lt;a href=&quot;http://www.nytimes.com/2015/01/11/magazine/sebastian-seungs-quest-to-map-the-human-brain.html&quot;&gt;connections&lt;/a&gt;. The idea is that, if we can get enough careful measurements all together, then we can have computers search for patterns that explain as much of the data as possible.&lt;/p&gt;

&lt;p&gt;This approach would have made perfect sense to &lt;a href=&quot;http://en.wikipedia.org/wiki/Tycho_Brahe&quot;&gt;Tycho Brahe&lt;/a&gt;, a Danish astronomer born in 1546. Although people have been studying the skies since the dawn of civilization, Brahe was the first to make a detailed catalog of stellar and planetary positions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.fis.cinvestav.mx/%7Elmontano/sciam/scientificamerican0114-72.pdf&quot;&gt;Scientific American&amp;#39;s description&lt;/a&gt; of his research program makes it clear that this was one of the first Big Data science projects:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Brahe was a towering figure. He ran a huge research program with a castlelike observatory, a NASA-like budget, and the finest instruments and best assistants money could buy. [...] Harvard University historian Owen Gingerich often illustrates Brahe’s importance with a mid-17th-century compilation by Albert Curtius of all astronomical data gathered since antiquity: the great bulk of two millennia’s worth of data came from Brahe.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Brahe then announced a model of planetary motion that fit his vast dataset exactly. You could use it to predict precisely where the stars and planets would be in the sky tomorrow. It relied on a fancy &lt;a href=&quot;http://mathworld.wolfram.com/ProsthaphaeresisFormulas.html&quot;&gt;prosthaphaeresis&lt;/a&gt; algorithm that allowed for the computation of a massive number of multiplications. The only problem was that it was deeply, fundamentally wrong.&lt;/p&gt;

&lt;p&gt;It was called the Geoheliocentric Model, since it proposed that the sun orbited the stationary Earth and the other planets orbited the sun. It was attractive on philosophical, scientific, and intuitive grounds (of course the Earth isn&amp;#39;t moving, what could possibly power such a fast motion of such a heavy object?). And it illustrates a critical problem with the data-mining approach to science: just because you have a model that predicts a pattern doesn&amp;#39;t mean that the model corresponds to reality.&lt;/p&gt;

&lt;p&gt;What might be needed is not just more data, or more precise data, but new hypotheses that drive the collection of entirely different types of data. It doesn&amp;#39;t mean that Big Data isn&amp;#39;t going to be part of the solution (most neuroimaging datasets have been laughably small so far), but simply performing pattern recognition on larger and larger datasets doesn&amp;#39;t guarantee that we&amp;#39;re getting closer to the truth. The geoheliocentric model was eventually brought down not with bigger datasets, but by a targeted experiment looking at small annual &lt;a href=&quot;http://en.wikipedia.org/wiki/Aberration_of_light&quot;&gt;patterns of stellar motion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Interestingly, there is a clear counterexample to my argument in the work of &lt;a href=&quot;http://web.mit.edu/yamins/www/&quot;&gt;Dan Yamins&lt;/a&gt;, a postdoc with &lt;a href=&quot;http://dicarlolab.mit.edu/&quot;&gt;Jim DiCarlo&lt;/a&gt;. Dan has shown that a neural network model that learns to label objects in a large set of images ends up looking a lot like the visual processing regions of the brain (in terms of its functional properties). This is surprising, since you could imagine that there might be lots of other ways to understand images.&lt;/p&gt;

&lt;p&gt;I wonder if this works because the brain is itself a Big Data mining machine, trained up through childhood to build models of our experiences. Then finding the strongest patterns in big datasets of experiences (images, videos, audio) might come up with the same solution as the brain. Or maybe our neural network models are starting to approximate the broad functional properties of the brain, which makes them a good hypothesis-driven model for finding patterns (rather than just blind data mining). As &lt;a href=&quot;http://www.psychology.ucsd.edu/people/profiles/jwixted.html&quot;&gt;John Wixted&lt;/a&gt; stressed at the &lt;a href=&quot;http://memory.psych.upenn.edu/CEMS_2015&quot;&gt;CEMS&lt;/a&gt; conference last week, hypothesis-free data anlysis has a seductive purity, but the true value of datasets (regardless of their size) comes only through the lens of carefully constructred ideas.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Seven things I learned as a PhD student</title>
   <link href="http://blog.chrisbaldassano.com//2015/03/31/phdadvice/"/>
   <updated>2015-03-31T00:00:00-04:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/03/31/phdadvice</id>
   <content type="html">&lt;p&gt;Doing great research is tough. There are so many factors outside of your control: experiments not panning out, unfair reviewers, competing labs, limited funding sources. I&amp;#39;ve tried to distill down some of the strategies that worked well for me and my labmates (these are most relevant to my background in science/engineering, but some might apply to other fields as well):&lt;/p&gt;

&lt;h2&gt;Get your hands dirty&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/frizzle.jpg&quot; alt=&quot;Magic School Bus&quot;&gt;&lt;/p&gt;

&lt;p&gt;Some early grad students get stuck in a loop of doing a lot of general talking about the kind of things they want to work on, but never really get started. Taking time to learn and plan your experiments is great, but there are a lot of things you can&amp;#39;t learn without diving into real data. You&amp;#39;re almost certainly going to mess things up the first one or two (or twenty) times, so start making mistakes as soon as possible. Having a deeper understanding of the data you&amp;#39;re dealing with will be invaluable in driving the kinds of questions you&amp;#39;ll ask and the design of your experiments.&lt;/p&gt;

&lt;h2&gt;Investigate things that don&amp;#39;t make sense&lt;/h2&gt;

&lt;p&gt;When you&amp;#39;re looking at the results of an analysis, often there will be something that just doesn&amp;#39;t quite line up: there&amp;#39;s one value of 1.01 when the maximum measurement should be 1, two data points are exactly on top of one another, 1% of the data points are giving &amp;quot;NaN&amp;quot;s. It&amp;#39;s easy to just brush these under the rug (&amp;quot;it&amp;#39;s just a couple datapoints&amp;quot;), but getting to the bottom of these is critical. Often they will reveal some flaw in your analysis that might mean all your results are invalid, or (if you&amp;#39;re lucky!) they might point to some new science hiding being an unexpected pattern in the data.&lt;/p&gt;

&lt;h2&gt;Explore, then replicate&lt;/h2&gt;

&lt;p&gt;The best way to approach an unfamiliar problem is to first collect (or extract from your full data) a pilot dataset, and start looking for patterns. You don&amp;#39;t need to be rigorous about statistics, multiple comparisons, or model complexity - what are the strongest signals you can find? Are there ways of transforming the data that make it more amenable to your models? Then, once you&amp;#39;ve optimized your analysis, you apply it to new (or held-out) data, and meticulously measure how well it performs. If you do your playing directly on the data, it&amp;#39;s very easy to start fooling yourself about what&amp;#39;s really there and what you just want to be there. &lt;/p&gt;

&lt;h2&gt;Realize that you&amp;#39;re in the big leagues&lt;/h2&gt;

&lt;p&gt;Throughout school, you&amp;#39;ve always been measured against your peers - your kindergarten macaroni crafts earned you a gold star because they were impressive for your experience level, not because they were competitive with a typical exhibit at the Louvre. In your first year of grad school, you are now competing with professional scientists who have been in the field for 40 years. This is intimidating (and one of the reasons why you start out being mentored by senior students and faculty members), but also exciting. You are on the front lines of scientific knowledge, answering real problems that no one has ever figured out before.&lt;/p&gt;

&lt;h2&gt;Know more than your advisor&lt;/h2&gt;

&lt;p&gt;This might sound contradictory to the previous point, since your advisor has a many-year head start in understanding your field, and you can&amp;#39;t hope to have more total knowledge by the end of your PhD. But for the particular project you&amp;#39;re working on, you should be finding papers on your own and reading everything you can. Publishing a paper that advances the field is going to require knowing more about that topic than anyone in the world, including your advisor.&lt;/p&gt;

&lt;h2&gt;Keep an end-of-the-day journal&lt;/h2&gt;

&lt;p&gt;Completing a PhD requires extremely long-term, self-guided planning, and it&amp;#39;s easy to lose track of what you should be working on and what the critical next steps are. Different people have different solutions for this, but my favorite strategy was to take 10 minutes at the end of the day and write (in a physical, dead-tree notebook) a couple bullet points about what I did that day and what the next steps should be. This forces you to take stock of your current goals, gives you a little morale boost (especially when you can look back over the past week and remind yourself that you really did make progress), and lets you pick up where you left off when you come back to your projects (possibly days later).&lt;/p&gt;

&lt;h2&gt;Drink Water&lt;/h2&gt;

&lt;p&gt;Taking care of your physical health is often the first thing to go when stress sets in, but this is a sure way to completely derail your research career. Drinking more water is an easy fix for most grad students - you can avoid kidney stones (I learned that the hard way), you&amp;#39;ll eat less junk, and having more bathroom breaks makes sure you take some breaks from your chair (no fitbit required). Some other no-brainers are to make sure you have an ergonomic typing setup (I know multiple PhDs that had to go on leave due to RSI) and keep a reasonable sleep schedule.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A simple model for growing a brain</title>
   <link href="http://blog.chrisbaldassano.com//2015/03/23/networkmodel/"/>
   <updated>2015-03-23T00:00:00-04:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/03/23/networkmodel</id>
   <content type="html">&lt;p&gt;Today I had a chance to read &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/25368200&quot;&gt;a paper by Song, Kennedy, and Wang&lt;/a&gt; about their model for explaining how brains are wired up at a high level (explaining how areas are connected, not the detailed wiring of neural circuits). It&amp;#39;s very simple, but manages to capture some complicated aspects of brain structure.&lt;/p&gt;

&lt;p&gt;The goal of the model is to offer some explanation for area-to-area connection patterns across species (humans, monkeys, mice). For example, the human connection matrix looks like this (from their supplementary material):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/connmat.png&quot; alt=&quot;Connection Matrix&quot;&gt;&lt;/p&gt;

&lt;p&gt;Gray squares show pairs of regions that (we think) are connected to each other. This looks complicated, and it is - every region has a different connection pattern, some are similar to each other (neighboring rows), some are very dissimilar, some regions (rows) have lots of connections and others have few, etc. The Song et al. paper starts by discussing the features of these matrices that seem predictable and similar across species, but the part I found more exciting was their proposed model for how you would grow a brain with these properties.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/connmodel.jpg&quot; alt=&quot;Connection Model&quot;&gt;&lt;/p&gt;

&lt;p&gt;This figure from their paper shows the setup. You first randomly choose a bunch of points as the centers of brain regions and randomly create a bunch of neurons, assigning each neuron to belong to the closest region center (A-C). Then, to grow a connection out from a neuron, you pick a direction as a weighted average of the directions to all region centers (with closer regions weighted more heavily), and then grow in that direction a random amount (with short connections more likely than long connections) (D-G). That&amp;#39;s it! Every neuron grows without talking to any other neuron, and they are not even really aiming anywhere in particular.&lt;/p&gt;

&lt;p&gt;This simple set of instructions is enough to produce some of the structures in real connectivity matrices, like relationships between how similar the connections are for two regions and how likely they are to be connected to one another. One of the take-aways is that spatial position is a very important feature for wiring brain regions - just adding a bias to connect close regions rather than distant regions is enough to explain a lot of the brain data. This sounds sort of obvious, but spatial position is often ignored in many analysis methods, and I&amp;#39;ve been recently proposing ways of incorporating spatial information for understanding connectivity at both the &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/25737822&quot;&gt;whole-brain&lt;/a&gt; and &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/22846660&quot;&gt;region&lt;/a&gt; level.&lt;/p&gt;

&lt;p&gt;There is a lot missing from this paper - it makes a literal &lt;a href=&quot;http://en.wikipedia.org/wiki/Spherical_cow&quot;&gt;&amp;quot;spherical cow&amp;quot;&lt;/a&gt; assumption that the brain is a solid ellipse, assumes that neurons are fired in straight lines like a laser, and doesn&amp;#39;t account for how brain size changes during development. But in some ways it makes the result even more impressive, since they can explain a lot of the data without using any of these details.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Hero's Powerpoint</title>
   <link href="http://blog.chrisbaldassano.com//2015/03/01/herostalk/"/>
   <updated>2015-03-01T00:00:00-05:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/03/01/herostalk</id>
   <content type="html">&lt;p&gt;What makes a good story? It turns out that, even among stories about very different things (romance, aliens, animated fluffy critters saving the world) there are some elements that are often the same. This topic is fascinating in its own right, but I find that it is also of great help for preparing academic talks. The goals of a talk (to keep the listener engaged and excited) are the same as those of a storyteller, and so it can be helpful to borrow some tips from the successful stories of the past few thousand years.&lt;/p&gt;

&lt;p&gt;The idea of a repeated story structure that has existed since antiquity (the &amp;quot;hero&amp;#39;s journey&amp;quot;) was first popularized by &lt;a href=&quot;http://en.wikipedia.org/wiki/Joseph_Campbell&quot;&gt;Joseph Campbell&lt;/a&gt;, and then later championed by &lt;a href=&quot;http://www.skepticfiles.org/atheist2/hero.htm&quot;&gt;Chris Vogler&lt;/a&gt;. What follows is my personal summary of the major parts of this framework, as an amateur cinephile. I&amp;#39;ve illustrated each part with images from Frozen, Iron Man, and The Matrix, which all stick closely to this formula.&lt;/p&gt;

&lt;h2&gt;Structure of the journey&lt;/h2&gt;

&lt;h3&gt;The ordinary world&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/hero/setting.png&quot; alt=&quot;The ordinary world&quot;&gt;
We&amp;#39;re introduced to the protagonist&amp;#39;s typical life. Things have been going the same way for some time, and the world is familiar. However, there are some questions or tensions lurking beneath the surface.&lt;/p&gt;

&lt;h3&gt;The breaking point&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/hero/breaking.png&quot; alt=&quot;The breaking point&quot;&gt;
The minor or unseen issues in the world burst to the forefront, disrupting the protagonist&amp;#39;s world in a catastrophic way. Life can no longer continue as it has in the past.&lt;/p&gt;

&lt;h3&gt;Into the wild&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/hero/wild.png&quot; alt=&quot;Into the wild&quot;&gt;
As a result, the hero is forced out into an unfamiliar world (physically or metaphorically). Their previous expertise and relationships are useless here, and they have no clear plan of action.&lt;/p&gt;

&lt;h3&gt;The mentor&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/hero/mentor.png&quot; alt=&quot;The mentor&quot;&gt;
The protagonist meets someone in the wild who begins to guide their path. This is someone that they never would have met in their ordinary world, and whose advice they might have previously dismissed. They begin to gain insight about themselves and their new environment.&lt;/p&gt;

&lt;h3&gt;Formulating the plan&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/hero/plan.png&quot; alt=&quot;Formulating the plan&quot;&gt;
With the mentor&amp;#39;s help, the protagonist understands their final goal and formulates a plan to achieve it. They often embark on this quest alone, without the mentor&amp;#39;s help, but with new knowledge gained from their time in the wild.&lt;/p&gt;

&lt;h3&gt;The failure of the plan&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/hero/failure.png&quot; alt=&quot;The failure of the plan&quot;&gt;
Despite their careful planning, the plan fails. The hero was missing critical information, about their own abilities or the loyalties of their supposed allies. All appears lost, the hero is dead or near death, and the final goal appears impossible to recover.&lt;/p&gt;

&lt;h3&gt;Seizing the sword&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/hero/seize.png&quot; alt=&quot;Seizing the sword&quot;&gt;
Miraculously, the hero is able to recover and continues the fight. A new plan is put into action, and the protagonist discovers new sources of strength. The enemy is defeated and goal achieved, albeit at great cost.&lt;/p&gt;

&lt;h3&gt;The return&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/public/hero/ending.png&quot; alt=&quot;The return&quot;&gt;
The hero returns to their ordinary world reborn, and we see the consequences of their quest. The old stability is replaced with a new one, and the hero sees old relationships in a new light.&lt;/p&gt;

&lt;h2&gt;Why we love the journey&lt;/h2&gt;

&lt;p&gt;This story structure is so common because it has a number of nice features that help keep the audience engaged:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Background information is only required at the beginning.&lt;/strong&gt; In order to follow the story and empathize with the hero, we need to be able to understand their feelings and actions. Doing this in the &amp;quot;ordinary world&amp;quot; is difficult, since the hero knows many things that that the audience does not know. By clearly designating that most of the story takes place in unfamiliar territory, we share the hero&amp;#39;s confusions and assumptions, and the audience doesn&amp;#39;t need to be constantly brought up to speed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;There is a clear call to action.&lt;/strong&gt; The problem faced by the hero is brought sharply to a breaking point, and there is no choice but to face it. Keeping things as they are is literally not an option.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Help from an unlikely source.&lt;/strong&gt; There are always &amp;quot;typical&amp;quot; ways of dealing with problems, such as just getting a bigger army or working harder. The mentor teaches an entirely new approach that runs counter to traditional wisdom in some way.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The quest is near impossible.&lt;/strong&gt; The failure of the hero&amp;#39;s plan emphasizes that the problem being solved is extremely difficult, and that even with all their new knowledge and training they couldn&amp;#39;t do it on their first try. When they do accomplish the task, it&amp;#39;s only through some incredible force of will. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The story has a impact beyond the protagonist.&lt;/strong&gt; The stakes go beyond the selfish desires of the hero, and the ordinary world is no longer the same after the events of the story.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Takeaways for presentations&lt;/h2&gt;

&lt;p&gt;If we want to make a talk into a good story, we should respect these same rules. Specifically:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Background information needs to be given up front, and limited to what is strictly necessary. The audience needs to understand the current landscape as quickly as possible, and the rest of the talk should be new information that you are adding to the field.&lt;/li&gt;
&lt;li&gt;Talks need to have as sense of urgency - it&amp;#39;s not enough that something simply hasn&amp;#39;t been studied before. Why is there an immediate, desperate need for a breakthrough in this area?&lt;/li&gt;
&lt;li&gt;It&amp;#39;s more interesting when insights come from a surprising source, such as a disconnected field or forgotten research from the past. What did you bring to the table that no one has previously tried?&lt;/li&gt;
&lt;li&gt;Talks shouldn&amp;#39;t shy away from showing how difficult a result was to achieve. For example, if presenting a complicated system you engineered, walk the audience through your thought process - what did you try first, and why did it fail? Problems that can be solved on the first try are generally not that interesting.&lt;/li&gt;
&lt;li&gt;Spend time in your conclusion talking about the impact of these results on the field in general. How does this change the way people have been thinking about a problem, or enable a new line of research?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course not all talks can use all of these elements, and this is only a starting point (since it says nothing about how to communicate the specific technical content). But we should all strive to take the audience on a journey with us during our presentation - make it an exciting one!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Are you thinking what I'm thinking?</title>
   <link href="http://blog.chrisbaldassano.com//2015/02/24/conchords/"/>
   <updated>2015-02-24T00:00:00-05:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/02/24/conchords</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/blog/public/fotc.jpg&quot; alt=&quot;Jemaine and Bret&quot;&gt;&lt;/p&gt;

&lt;p&gt;Flight of the Conchords has been one of my favorite comedy bands for years, mixing general silliness with some insightful satire on life and relationships. They have a whole catalog of great songs (&lt;a href=&quot;https://www.youtube.com/watch?v=BNC61-OOPdA&quot;&gt;one of which&lt;/a&gt; is highly relevant to the title of this blog), but one of my favorite exchanges is at 1:00 into their song &amp;quot;We&amp;#39;re Both in Love with a Sexy Lady&amp;quot;:&lt;/p&gt;

&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/OOvQ1EvMy5k?start=60&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;blockquote&gt;
&lt;p&gt;Bret: Are you thinking what I&amp;#39;m thinking?&lt;/p&gt;

&lt;p&gt;Jemaine: No, I&amp;#39;m thinking what I&amp;#39;m thinking.&lt;/p&gt;

&lt;p&gt;Bret: So you&amp;#39;re not thinking what I&amp;#39;m thinking?&lt;/p&gt;

&lt;p&gt;Jemaine: No, &amp;#39;cause you&amp;#39;re thinking I&amp;#39;m thinking what you&amp;#39;re thinking.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Both of Jemaine&amp;#39;s lines touch on deep concepts in cognitive science, so let&amp;#39;s take them one at a time:&lt;/p&gt;

&lt;h2&gt;I&amp;#39;m thinking what I&amp;#39;m thinking&lt;/h2&gt;

&lt;p&gt;Jemaine&amp;#39;s first objection is that of course he&amp;#39;s not thinking Bret&amp;#39;s thoughts, he&amp;#39;s thinking his own! It is actually very unclear exactly what it means for two people to have &amp;quot;the same&amp;quot; thought. For systems with a standardized architecture like computers, it makes perfect sense to say that a file on my computer is the same as a file on your computer - each file consists of an ordered list of 0s and 1s, and we can just compare these two lists to see if they are the same. For neural systems, however, we can&amp;#39;t hope to do this same kind of comparison. It seems implausible that two people could have a one-to-one correspondence between the neurons in their brains and have the same pattern of firing on these neurons. Is there a way to summarize the current state of the brain in a way that abstracts away from the architecture of a particular person&amp;#39;s brain, and allows us to compare thoughts between people?&lt;/p&gt;

&lt;p&gt;The answer is, of course, that humans invented such a system about a hundred thousand years ago, called &lt;em&gt;language.&lt;/em&gt; The fact that we are able to convert our thoughts into words, and decode others&amp;#39; words into our own thoughts, is an incredible feat. In a sense, we are all bilingual - our internal thoughts are represented by patterns of brain activity unique to our own brains, and we can translate these to and from a language that is understood by others. This sentence is some dynamic state in my brain which I have compressed into a string of characters, which your brain is then uncompressing to create a corresponding state in your brain. Language is not perfect, and we can sometimes struggle to translate our thoughts into words, but considering the complexity of the human brain (with 100 trillion connections between neurons) we can do an impressive job of copying thoughts between brains using a one-dimensional channel of text or speech.&lt;/p&gt;

&lt;p&gt;There has been some early work on building computer systems which can perform this same kind of task, producing a natural language description of some complex internal representation of information. For example, &lt;a href=&quot;http://www.nytimes.com/2014/11/18/science/researchers-announce-breakthrough-in-content-recognition-software.html?ref=science&quot;&gt;both Stanford and Google&lt;/a&gt; have built artificial neural network systems that can look at an image, produce some numerical vector representation of its content, and then translate that representation into a sentence that humans can understand. My labmate Andrej Karpathy has put up a &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/&quot;&gt;cool demo&lt;/a&gt; of sample sentences (you can refresh to get new ones) to show how well this is currently working. It&amp;#39;s clearly far below human performance, but describes a surprising number of images correctly.&lt;/p&gt;

&lt;p&gt;Going back to human brains: though we can&amp;#39;t expect a neuron-level correspondence between brains, might there be a more coarse similarity at the millimeter scale? Maybe the average activity of a clump of ~10,000 neurons in my brain when I think about dogs looks like the average activity in a similarly-positioned clump of neurons in your brain. There have been &lt;a href=&quot;http://hlab.princeton.edu/Papers/Hasson_et_al_TiCS_2012_F.pdf&quot;&gt;a great deal&lt;/a&gt; of interesting neuroimaging experiments testing this hypothesis, and it seems that there are a number of brain regions with this property. In fact, the more in-sync a listener&amp;#39;s brain is to the speaker&amp;#39;s, the better they comprehend what the speaker is saying.&lt;/p&gt;

&lt;h2&gt;You&amp;#39;re thinking I&amp;#39;m thinking what you&amp;#39;re thinking.&lt;/h2&gt;

&lt;p&gt;Jemaine&amp;#39;s second objection is that Bret is currently thinking about a model of Jemaine&amp;#39;s mind, and since Jemaine isn&amp;#39;t thinking about his own mind their thoughts can&amp;#39;t be the same. The ability to make these kind of statements, about the mental states of others, is called having a &lt;a href=&quot;http://en.wikipedia.org/wiki/Theory_of_mind&quot;&gt;Theory of Mind&lt;/a&gt;. In fact, understanding Jemaine&amp;#39;s sentence is a fourth-order theory of mind task for us as listeners: Jemaine thinks that Bret thinks that Jemaine is thinking what Bret is thinking. The fact that we can even comprehend this sentence is remarkable. No other animal (as far as we know) has the ability to build these high-order models of the thoughts of others, and humans require years of practice.&lt;/p&gt;

&lt;p&gt;If you want to test how good your theory of mind skills are, take a look at one of the questions used in &lt;a href=&quot;https://www.staff.ncl.ac.uk/daniel.nettle/liddlenettle.pdf&quot;&gt;studies of schoolchildren&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There was a little girl called Anna who had a big problem on her mind. It was her mums birthday the very next day. Anna wanted to get her mum a birthday present, but she just didn&amp;#39;t know what to buy. She thought and she thought. Tomorrow was nearly here! Anna remembered that her brother, Ben, had already asked mum what
mum would like most of all for her birthday. Ben was out riding his bike so Anna decided to look around his room to see if she could find what present he had got for mum. Anna went in and found a big bunch of beautiful flowers with a little card that said: &amp;#39;Happy Birthday Mum, love from Ben.&amp;#39; Anna thought to herself &amp;#39;mum must want flowers for her birthday!&amp;#39; Just as Anna was leaving the room, Ben was coming up the stairs, but he didn&amp;#39;t see Anna leaving his room. Anna didn&amp;#39;t want Ben to know that she had been snooping round his room, so she said to Ben: &amp;quot;Ben, have you got mum a birthday present?&amp;quot; Ben thought for a minute, he didn&amp;#39;t want Anna to copy him and get mum the same present, so he said: &amp;quot;Yes, Anna, I have. I have got mum some perfume. What have you got for her?&amp;quot; Anna replied: &amp;quot;Erm, nothing yet, Ben&amp;quot; and she walked away.&lt;/p&gt;

&lt;p&gt;Which is true?&lt;/p&gt;

&lt;p&gt;a) Ben thinks that Anna believes that he knows that Mum wants perfume for her birthday.&lt;/p&gt;

&lt;p&gt;b) Ben thinks that Anna knows that he knows that mum wants flowers for her birthday. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;11 year-olds are unable to answer this question, while most adults can. It requires tracking the knowledge states of multiple people, and understanding how each is trying to deceive the other. (In case you&amp;#39;re still struggling, the answer is (a)).&lt;/p&gt;

&lt;p&gt;There have been some efforts to incorporate basic theory of mind into AI assistants like Siri, Google Now, and Cortana, in the sense that they can keep track of basic context: if you ask about the weather and then say &amp;quot;what about this weekend?&amp;quot;, these systems will understand that you&amp;#39;re still thinking about the weather and interpret your question in this context. However, I don&amp;#39;t know of any systems that really try to build a deep understanding of the user&amp;#39;s mental state or tackle higher-order theory of mind tasks. I predict that in the near future we&amp;#39;ll start seeing assistants that keep track not only of what you should know, but also what you currently do and don&amp;#39;t know, so that they can deliver information only when you need it. If my wife emails me to say that she&amp;#39;s moved up the time of our dinner reservation, and my personal AI sees that I haven&amp;#39;t read the email and haven&amp;#39;t left for the restaurant, it should guess that I am mistaken about my wife&amp;#39;s plans and interrupt what I&amp;#39;m working on. Perhaps a more useful measure of an AI&amp;#39;s conversational abilities than the Turing test is a theory-of-mind test: can an AI understand what we know, how we&amp;#39;re feeling, and what we want? Feeding it some Flight of the Conchords as training data might be a good place to start.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Jerry Kaplan&#58; The Law of Artificial Intelligence</title>
   <link href="http://blog.chrisbaldassano.com//2015/02/19/kaplan/"/>
   <updated>2015-02-19T00:00:00-05:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/02/19/kaplan</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/blog/public/kaplan.jpg&quot; alt=&quot;Kaplan&quot;&gt;&lt;/p&gt;

&lt;p&gt;Jerry Kaplan has been an interesting force in the Stanford AI community over the past couple years. He&amp;#39;s been a major player in Silicon Valley since the 80s, was one of the early pioneers of touch-sensitive tablets and online auctions, and wrote a book about his &lt;a href=&quot;http://www.amazon.com/Startup-A-Silicon-Valley-Adventure/dp/0140257314&quot;&gt;1987 pen-based operating system company&lt;/a&gt; (which was ahead of its time, unfortunately for the company). Recently, however, he seems to have a new mission of fostering a broader discussion of the history and philosophy of AI, especially on the Stanford campus. He has been giving a number of talks on these topics, and &lt;a href=&quot;http://web.stanford.edu/class/cs22/&quot;&gt;taught a class&lt;/a&gt; that brought in other AI speakers. &lt;/p&gt;

&lt;p&gt;His most recent talk, through the &lt;a href=&quot;https://www.law.stanford.edu/organizations/programs-and-centers/codex-the-stanford-center-for-legal-informatics&quot;&gt;Stanford CodeX center&lt;/a&gt;, was partially a plug for his upcoming book, &lt;a href=&quot;http://yalepress.yale.edu/yupbooks/book.asp?isbn=9780300213553&quot;&gt;&amp;quot;Humans Need Not Apply: A Guide to Wealth and Work in the Age of Artificial Intelligence&amp;quot;&lt;/a&gt; but focused specifically on how the legal system should handle the rise of autonomous systems. He draws a distinction between a system being &amp;quot;conscious&amp;quot; (which is far off for machines, and more of a philosophical question) and a system being a &amp;quot;moral agent&amp;quot; that is legally considered as an &amp;quot;artificial person.&amp;quot; Arguably corporations already fall under this definition, since they have rights (e.g. free speech) and can be charged with legal action independently from their employees (e.g. in the BP Deepwater Horizon spill).&lt;/p&gt;

&lt;p&gt;Can an AI be a moral agent? Jerry argues that systems like autonomous cars are able to predict the consequences of their actions and are in control of those actions (without requiring human approval), so they meet at least a basic definition of moral agency. As has been &lt;a href=&quot;http://www.theatlantic.com/technology/archive/2013/10/the-ethics-of-autonomous-cars/280360/&quot;&gt;discussed by others&lt;/a&gt;, self-driving cars will have to make decisions analogous to the philosophical &amp;quot;trolley problem,&amp;quot; in which every possible action results in harm to humans and value judgments must be made. Since AIs can (implicitly or explicitly) have some encoded moral system, they should bear some responsibility for their actions.&lt;/p&gt;

&lt;p&gt;He proposed a few ways of actually enforcing this in practice. The simplest would be some type of licensing system, in which every AI system meeting some threshold of complexity would need to be registered with the government. If an AI is misbehaving in some way, such as a driverless taxi driving recklessly fast in hopes of a better tip, then its license would be revoked. The AI might just be destroyed, or if we think that the problem is fixable then it could be &amp;quot;rehabilitated&amp;quot; e.g. with new training data. There are many possible complications here (I asked him about how this would work if the AI is partially distributed in the cloud), but this general approach makes sense.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m happy that we&amp;#39;re handing over more and more control to AIs, freeing up humans from mundane tasks and probably outperforming them in many areas (though this will require some restructuring of the economy, which is a topic for another post). There are, however, some very practical problems that we need to address sooner than later - I&amp;#39;m glad to see that technically-minded people like Jerry are diving into these issues.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Michael Gazzaniga&#58; Tales from Both Sides of the Brain</title>
   <link href="http://blog.chrisbaldassano.com//2015/02/13/gazzaniga/"/>
   <updated>2015-02-13T00:00:00-05:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/02/13/gazzaniga</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/blog/public/gazzaniga.jpg&quot; alt=&quot;Gazzaniga&quot;&gt;&lt;/p&gt;

&lt;p&gt;Michael Gazzaniga&amp;#39;s new book is &lt;a href=&quot;http://www.amazon.com/Tales-Both-Sides-Brain-Neuroscience/dp/0062228803&quot;&gt;Tales from Both Sides of the Brain: A Life in Neuroscience&lt;/a&gt;, which is a memoir about his scientific study of split-brain patients. If you&amp;#39;re unfamiliar with this work, people who have had their two brain hemispheres surgically separated (for medical reasons) show some &lt;a href=&quot;http://www.nature.com/news/the-split-brain-a-tale-of-two-halves-1.10213&quot;&gt;amazingly interesting behaviors&lt;/a&gt;, which raise deep questions about consciousness and free will. Gazzaniga set out to simply write a standard popular science book about his research, but ended up publishing a much more autobiographical book that discusses the actual process of scientific discovery. As he writes in the preface:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Most attempts at capturing the history of a scientific saga describe the seemingly ordered and logical way an idea developed... I want to present a different picture: science carried out in friendship, where discoveries are deeply embedded in the social relations of people from all walks of life. It is a wonderful way of life, spending one&amp;#39;s years with smart people, puzzling over the mysteries and surprises of nature.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a fact of scientific research that isn&amp;#39;t often highlighted - personalities and relationships are central to the scientific community, even though it rarely looks that way to the public.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;One of the main takeaways from the talk for me was how young the field of neuroscience is, compared to most other hard sciences. Gazzaniga started his work in the early 1960s, when &lt;a href=&quot;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(15)60224-0/fulltext&quot;&gt;the term &amp;quot;neuroscience&amp;quot; barely even existed&lt;/a&gt;. The fact that the founders of the field are still around should remind us that neuroscience is in its infancy, and we still know very little. Instead of being depressing, it&amp;#39;s kind of liberating - it means that most of the big ideas and unifying theories are still out there to be discovered, and we should take everything we &amp;quot;know&amp;quot; so far with a grain of salt.&lt;/p&gt;

&lt;p&gt;Gazzaniga said that when he started at Caltech, there were basically two rules:  &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Do important things: don&amp;#39;t do an experiment just because it&amp;#39;s a new thing to try.&lt;/li&gt;
&lt;li&gt;If it&amp;#39;s important, just do it: don&amp;#39;t spend too much time planning and worrying about exactly the right way to test something. In my experience, it&amp;#39;s often hard to even tell ahead of time what the hard parts are going to be, and trying things out is a must faster way to make the experiment better.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Given the early state of the field, neuroscientists should take these to heart. We&amp;#39;re still looking for the big principles of the brain and mind. Now is not the time to be polishing up the details of our models - we&amp;#39;re still brainstorming.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hello World</title>
   <link href="http://blog.chrisbaldassano.com//2015/01/27/helloworld/"/>
   <updated>2015-01-27T00:00:00-05:00</updated>
   <id>http://blog.chrisbaldassano.com//2015/01/27/helloworld</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/blog/public/NG2-flare.jpg&quot; alt=&quot;Greg Dunn Glial Flare&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
  I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines. -Claude Shannon, Omni Magazine (1987)
&lt;/div&gt;

&lt;p&gt;I&amp;#39;m a soon-to-be PhD who has spent the last few years at the intersection of machine learning (i.e. building systems to model complex data) and human neuroscience (i.e. trying to collect data on a complex system). I believe that both of these fields are going to have a profound impact in the coming decades, in terms of how we interact with technology, how we communicate, and how we see ourselves. In contrast to my &lt;a href=&quot;http://www.chrisbaldassano.com&quot;&gt;published papers&lt;/a&gt;, in which I have to actually support my points with hard evidence, this blog is an opportunity to speculate about where we are and what&amp;#39;s coming next.&lt;/p&gt;
</content>
 </entry>
 

</feed>
