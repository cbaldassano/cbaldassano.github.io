<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Rooting for the machines &middot; A Blog by Chris Baldassano
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">


  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="Blog feed" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15871783-1', 'auto');
  ga('send', 'pageview');

  </script>

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@ChrisBaldassano">
  <meta name="twitter:creator" content="@ChrisBaldassano">
  
    <meta name="twitter:title" content="Home">
  
  
    <meta name="twitter:url" content="http://localhost:4000/page3/">
  
  
    <meta name="twitter:description" content="A blog about real minds, artificial minds, using artificial minds to understand real minds, and using real minds to inspire artificial minds">
  


</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A blog about real minds, artificial minds, using artificial minds to understand real minds, and using real minds to inspire artificial minds</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://www.chrisbaldassano.com">Main</a>
    <a class="sidebar-nav-item" href="/blog/">Blog</a>


    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/blog/archive/">Blog Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    

    <a class="sidebar-nav-item" href="/blog/atom.xml">Atom feed</a>
    
  </nav>

  <div class="sidebar-item">
    <p>
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/blog" title="Home">Rooting for the machines</a>
            <small>A Blog by Chris Baldassano</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2015/12/01/race/">
        Just how Amazing is The Amazing Race?
      </a>
    </h1>

    <span class="post-date">01 Dec 2015</span>

    <h2>The Amazing Race</h2>

<p>The <a href="https://en.wikipedia.org/wiki/The_Amazing_Race">Amazing Race</a> is one of the few reality TV shows that managed to survive the bubble of the early 2000s, with good reason. Rather than just trying to play up interpersonal dramas (though there is some of that too), it is set up like a traditional game show with a series of competitions between teams of two, who travel to different cities throughout the world over the course of the show. Eleven teams start out the race, and typically the last team to finish each day&#39;s challenges gets booted from the show until only three teams are left. These three teams then have a final day of competition, with the winner being awarded $1 million.</p>

<p>Winning first place on any day before the last one doesn&#39;t matter much (though you get a small prize and some bragging rights), which is interesting, since it means that it is possible for the winning team to have never come in first place before the final leg. This got me wondering: if we think of the Race as an experiment which is trying to identify the best team, how good is it? What if we just gave teams a point for every first place win, and then saw which one got the most points, like a baseball series?</p>

<h2>Modeling the Race</h2>

<p>To try to answer this question, I build a simple model of the Race. I assume that each team has some fixed skill level (sampled from a standard normal distribution), and then on each leg their performance is the sum of this instrinc skill and some randomness (sampled from another normal with varying width). So every leg, the ranking of the teams will be their true skill ranking, plus some randomness (and there can be a lot of randomness on the race). Fans of the show will know that this is a very simplified model of the race (the legs aren&#39;t totally independent, the teams can strategically interfere with each other, etc.) but this captures the basic idea. I ran simulated races 10,000 times for each level of randomness.</p>

<p>We can measure how effective the Race was at picking a winner, by seeing what true skill rank the winning team had. So if the team with the highest skill (number 1) wins, that means the race did a good job. If a team with a low skill rank (like 10) wins, then the race did a very bad job of picking the million-dollar winner. This plot shows the rank of the winning team, compared to chance ((1+11)/2=6).</p>

<p><img src="/blog/public/race/elim_rank.png" alt="Winning rank using loser-elimination"></p>

<p>This actually looks surprisingly good! Even at with lots of leg randomness (more than the actual skill difference between the teams) a team with a relatively high rank tends to win. Once the randomness gets to be an order of magnitude bigger than the differences between teams, the winner starts getting close to random.</p>

<h2>Improving the Race</h2>

<p>But how good is this relative to a simpler kind of competition, where the winner is the team with the most first-place wins? Rather than eliminating teams, all teams race all 9 legs, and the team coming in first the most wins the prize (ties are broken based on which team won most recently). Would this do better or worse?</p>

<p><img src="/blog/public/race/elim_first_rank.png" alt="Winning rank using loser-elimination or first place wins"></p>

<p>Turns out this is a little bit better! In general the rank of the winning team tends to be higher, meaning that a &quot;more deserving&quot; team won the money. But the size of the gap depends on how much randomness there is in each leg of the race. Which point along these curves corresponds to the actual TV show?</p>

<p>To answer this, I took the per-leg rankings from the <a href="http://amazingrace.wikia.com/wiki/Main_Page">Amazing Race Wikia</a> from the past 10 seasons. Yes, there are people way more obsessed with this show than me, who have been together databases of stats from each season. I measured how consistent the rankings were from each leg of the race. If there wasn&#39;t any randomness, we&#39;d expect these to have a perfect (Kendall) correlation, while if each leg is just craziness for all teams then the correlation should be near zero. I found that this correlation varied a bit across seasons, but had a mean of 0.0992. Comparing this to the same calculation from the model, this corresponds to a noise level of about sigma=2.2.</p>

<p><img src="/blog/public/race/tau_rank.png" alt="Leg ranking correlations for each noise level"></p>

<p>At this level of randomness, there is about a 10% advantage for counting-first-places competition: 37.4% of the time it picks a better team to win the money, while 28.5% of the time the current elimination setup picks a better team (they pick the same team 34.1% of the time).</p>

<p><img src="/blog/public/race/frac_compare.png" alt="Comparison of methods at sigma=2.2"></p>

<p>Of course there are some disadvantages to counting first place wins: the requires all teams to run all legs (which is logistically difficult and means we get to know each team less) and the winner might be locked-in before the final leg (ruining the suspense of the grand finale they usually have set up for the final tasks). This is likely a general tradeoff in games like this, between being fair (making the right team more likely to win) and being exciting (keeping the winner more uncertain until the end). As a game show, The Amazing Race probably makes the right choice (entertainment over fairness) but for more serious matters (political debate performance?) maybe we should pay attention to the winner of each round rather than the loser.</p>

<p>All the MATLAB code and ranking data is available on <a href="https://bitbucket.org/cbaldassano/amazing-race">my bitbucket</a>.</p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2015/05/11/bigdata/">
        The Big Data bust of the 1500s&#58; lessons from the first scientific data miner
      </a>
    </h1>

    <span class="post-date">11 May 2015</span>

    <p><img src="/blog/public/geocentric.png" alt="Geoheliocentric Model"></p>

<p>The Big Data craze is in full swing within many scientific fields, especially neuroscience. Since we can&#39;t hope to understand the tangled web of the brain by looking at only one tiny piece, groups have started to amass huge datasets describing brain <a href="http://neurosynth.org/">function</a> and <a href="http://www.nytimes.com/2015/01/11/magazine/sebastian-seungs-quest-to-map-the-human-brain.html">connections</a>. The idea is that, if we can get enough careful measurements all together, then we can have computers search for patterns that explain as much of the data as possible.</p>

<p>This approach would have made perfect sense to <a href="http://en.wikipedia.org/wiki/Tycho_Brahe">Tycho Brahe</a>, a Danish astronomer born in 1546. Although people have been studying the skies since the dawn of civilization, Brahe was the first to make a detailed catalog of stellar and planetary positions.</p>

<p><a href="http://www.fis.cinvestav.mx/%7Elmontano/sciam/scientificamerican0114-72.pdf">Scientific American&#39;s description</a> of his research program makes it clear that this was one of the first Big Data science projects:</p>

<blockquote>
<p>Brahe was a towering figure. He ran a huge research program with a castlelike observatory, a NASA-like budget, and the finest instruments and best assistants money could buy. [...] Harvard University historian Owen Gingerich often illustrates Brahe’s importance with a mid-17th-century compilation by Albert Curtius of all astronomical data gathered since antiquity: the great bulk of two millennia’s worth of data came from Brahe.</p>
</blockquote>

<p>Brahe then announced a model of planetary motion that fit his vast dataset exactly. You could use it to predict precisely where the stars and planets would be in the sky tomorrow. It relied on a fancy <a href="http://mathworld.wolfram.com/ProsthaphaeresisFormulas.html">prosthaphaeresis</a> algorithm that allowed for the computation of a massive number of multiplications. The only problem was that it was deeply, fundamentally wrong.</p>

<p>It was called the Geoheliocentric Model, since it proposed that the sun orbited the stationary Earth and the other planets orbited the sun. It was attractive on philosophical, scientific, and intuitive grounds (of course the Earth isn&#39;t moving, what could possibly power such a fast motion of such a heavy object?). And it illustrates a critical problem with the data-mining approach to science: just because you have a model that predicts a pattern doesn&#39;t mean that the model corresponds to reality.</p>

<p>What might be needed is not just more data, or more precise data, but new hypotheses that drive the collection of entirely different types of data. It doesn&#39;t mean that Big Data isn&#39;t going to be part of the solution (most neuroimaging datasets have been laughably small so far), but simply performing pattern recognition on larger and larger datasets doesn&#39;t guarantee that we&#39;re getting closer to the truth. The geoheliocentric model was eventually brought down not with bigger datasets, but by a targeted experiment looking at small annual <a href="http://en.wikipedia.org/wiki/Aberration_of_light">patterns of stellar motion</a>.</p>

<p>Interestingly, there is a clear counterexample to my argument in the work of <a href="http://web.mit.edu/yamins/www/">Dan Yamins</a>, a postdoc with <a href="http://dicarlolab.mit.edu/">Jim DiCarlo</a>. Dan has shown that a neural network model that learns to label objects in a large set of images ends up looking a lot like the visual processing regions of the brain (in terms of its functional properties). This is surprising, since you could imagine that there might be lots of other ways to understand images.</p>

<p>I wonder if this works because the brain is itself a Big Data mining machine, trained up through childhood to build models of our experiences. Then finding the strongest patterns in big datasets of experiences (images, videos, audio) might come up with the same solution as the brain. Or maybe our neural network models are starting to approximate the broad functional properties of the brain, which makes them a good hypothesis-driven model for finding patterns (rather than just blind data mining). As <a href="http://www.psychology.ucsd.edu/people/profiles/jwixted.html">John Wixted</a> stressed at the <a href="http://memory.psych.upenn.edu/CEMS_2015">CEMS</a> conference last week, hypothesis-free data anlysis has a seductive purity, but the true value of datasets (regardless of their size) comes only through the lens of carefully constructred ideas.</p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2015/03/31/phdadvice/">
        Seven things I learned as a PhD student
      </a>
    </h1>

    <span class="post-date">31 Mar 2015</span>

    <p>Doing great research is tough. There are so many factors outside of your control: experiments not panning out, unfair reviewers, competing labs, limited funding sources. I&#39;ve tried to distill down some of the strategies that worked well for me and my labmates (these are most relevant to my background in science/engineering, but some might apply to other fields as well):</p>

<h2>Get your hands dirty</h2>

<p><img src="/blog/public/frizzle.jpg" alt="Magic School Bus"></p>

<p>Some early grad students get stuck in a loop of doing a lot of general talking about the kind of things they want to work on, but never really get started. Taking time to learn and plan your experiments is great, but there are a lot of things you can&#39;t learn without diving into real data. You&#39;re almost certainly going to mess things up the first one or two (or twenty) times, so start making mistakes as soon as possible. Having a deeper understanding of the data you&#39;re dealing with will be invaluable in driving the kinds of questions you&#39;ll ask and the design of your experiments.</p>

<h2>Investigate things that don&#39;t make sense</h2>

<p>When you&#39;re looking at the results of an analysis, often there will be something that just doesn&#39;t quite line up: there&#39;s one value of 1.01 when the maximum measurement should be 1, two data points are exactly on top of one another, 1% of the data points are giving &quot;NaN&quot;s. It&#39;s easy to just brush these under the rug (&quot;it&#39;s just a couple datapoints&quot;), but getting to the bottom of these is critical. Often they will reveal some flaw in your analysis that might mean all your results are invalid, or (if you&#39;re lucky!) they might point to some new science hiding being an unexpected pattern in the data.</p>

<h2>Explore, then replicate</h2>

<p>The best way to approach an unfamiliar problem is to first collect (or extract from your full data) a pilot dataset, and start looking for patterns. You don&#39;t need to be rigorous about statistics, multiple comparisons, or model complexity - what are the strongest signals you can find? Are there ways of transforming the data that make it more amenable to your models? Then, once you&#39;ve optimized your analysis, you apply it to new (or held-out) data, and meticulously measure how well it performs. If you do your playing directly on the data, it&#39;s very easy to start fooling yourself about what&#39;s really there and what you just want to be there. </p>

<h2>Realize that you&#39;re in the big leagues</h2>

<p>Throughout school, you&#39;ve always been measured against your peers - your kindergarten macaroni crafts earned you a gold star because they were impressive for your experience level, not because they were competitive with a typical exhibit at the Louvre. In your first year of grad school, you are now competing with professional scientists who have been in the field for 40 years. This is intimidating (and one of the reasons why you start out being mentored by senior students and faculty members), but also exciting. You are on the front lines of scientific knowledge, answering real problems that no one has ever figured out before.</p>

<h2>Know more than your advisor</h2>

<p>This might sound contradictory to the previous point, since your advisor has a many-year head start in understanding your field, and you can&#39;t hope to have more total knowledge by the end of your PhD. But for the particular project you&#39;re working on, you should be finding papers on your own and reading everything you can. Publishing a paper that advances the field is going to require knowing more about that topic than anyone in the world, including your advisor.</p>

<h2>Keep an end-of-the-day journal</h2>

<p>Completing a PhD requires extremely long-term, self-guided planning, and it&#39;s easy to lose track of what you should be working on and what the critical next steps are. Different people have different solutions for this, but my favorite strategy was to take 10 minutes at the end of the day and write (in a physical, dead-tree notebook) a couple bullet points about what I did that day and what the next steps should be. This forces you to take stock of your current goals, gives you a little morale boost (especially when you can look back over the past week and remind yourself that you really did make progress), and lets you pick up where you left off when you come back to your projects (possibly days later).</p>

<h2>Drink Water</h2>

<p>Taking care of your physical health is often the first thing to go when stress sets in, but this is a sure way to completely derail your research career. Drinking more water is an easy fix for most grad students - you can avoid kidney stones (I learned that the hard way), you&#39;ll eat less junk, and having more bathroom breaks makes sure you take some breaks from your chair (no fitbit required). Some other no-brainers are to make sure you have an ergonomic typing setup (I know multiple PhDs that had to go on leave due to RSI) and keep a reasonable sleep schedule.</p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2015/03/23/networkmodel/">
        A simple model for growing a brain
      </a>
    </h1>

    <span class="post-date">23 Mar 2015</span>

    <p>Today I had a chance to read <a href="http://www.ncbi.nlm.nih.gov/pubmed/25368200">a paper by Song, Kennedy, and Wang</a> about their model for explaining how brains are wired up at a high level (explaining how areas are connected, not the detailed wiring of neural circuits). It&#39;s very simple, but manages to capture some complicated aspects of brain structure.</p>

<p>The goal of the model is to offer some explanation for area-to-area connection patterns across species (humans, monkeys, mice). For example, the human connection matrix looks like this (from their supplementary material):</p>

<p><img src="/blog/public/connmat.png" alt="Connection Matrix"></p>

<p>Gray squares show pairs of regions that (we think) are connected to each other. This looks complicated, and it is - every region has a different connection pattern, some are similar to each other (neighboring rows), some are very dissimilar, some regions (rows) have lots of connections and others have few, etc. The Song et al. paper starts by discussing the features of these matrices that seem predictable and similar across species, but the part I found more exciting was their proposed model for how you would grow a brain with these properties.</p>

<p><img src="/blog/public/connmodel.jpg" alt="Connection Model"></p>

<p>This figure from their paper shows the setup. You first randomly choose a bunch of points as the centers of brain regions and randomly create a bunch of neurons, assigning each neuron to belong to the closest region center (A-C). Then, to grow a connection out from a neuron, you pick a direction as a weighted average of the directions to all region centers (with closer regions weighted more heavily), and then grow in that direction a random amount (with short connections more likely than long connections) (D-G). That&#39;s it! Every neuron grows without talking to any other neuron, and they are not even really aiming anywhere in particular.</p>

<p>This simple set of instructions is enough to produce some of the structures in real connectivity matrices, like relationships between how similar the connections are for two regions and how likely they are to be connected to one another. One of the take-aways is that spatial position is a very important feature for wiring brain regions - just adding a bias to connect close regions rather than distant regions is enough to explain a lot of the brain data. This sounds sort of obvious, but spatial position is often ignored in many analysis methods, and I&#39;ve been recently proposing ways of incorporating spatial information for understanding connectivity at both the <a href="http://www.ncbi.nlm.nih.gov/pubmed/25737822">whole-brain</a> and <a href="http://www.ncbi.nlm.nih.gov/pubmed/22846660">region</a> level.</p>

<p>There is a lot missing from this paper - it makes a literal <a href="http://en.wikipedia.org/wiki/Spherical_cow">&quot;spherical cow&quot;</a> assumption that the brain is a solid ellipse, assumes that neurons are fired in straight lines like a laser, and doesn&#39;t account for how brain size changes during development. But in some ways it makes the result even more impressive, since they can explain a lot of the data without using any of these details.</p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2015/03/01/herostalk/">
        The Hero's Powerpoint
      </a>
    </h1>

    <span class="post-date">01 Mar 2015</span>

    <p>What makes a good story? It turns out that, even among stories about very different things (romance, aliens, animated fluffy critters saving the world) there are some elements that are often the same. This topic is fascinating in its own right, but I find that it is also of great help for preparing academic talks. The goals of a talk (to keep the listener engaged and excited) are the same as those of a storyteller, and so it can be helpful to borrow some tips from the successful stories of the past few thousand years.</p>

<p>The idea of a repeated story structure that has existed since antiquity (the &quot;hero&#39;s journey&quot;) was first popularized by <a href="http://en.wikipedia.org/wiki/Joseph_Campbell">Joseph Campbell</a>, and then later championed by <a href="http://www.skepticfiles.org/atheist2/hero.htm">Chris Vogler</a>. What follows is my personal summary of the major parts of this framework, as an amateur cinephile. I&#39;ve illustrated each part with images from Frozen, Iron Man, and The Matrix, which all stick closely to this formula.</p>

<h2>Structure of the journey</h2>

<h3>The ordinary world</h3>

<p><img src="/blog/public/hero/setting.png" alt="The ordinary world">
We&#39;re introduced to the protagonist&#39;s typical life. Things have been going the same way for some time, and the world is familiar. However, there are some questions or tensions lurking beneath the surface.</p>

<h3>The breaking point</h3>

<p><img src="/blog/public/hero/breaking.png" alt="The breaking point">
The minor or unseen issues in the world burst to the forefront, disrupting the protagonist&#39;s world in a catastrophic way. Life can no longer continue as it has in the past.</p>

<h3>Into the wild</h3>

<p><img src="/blog/public/hero/wild.png" alt="Into the wild">
As a result, the hero is forced out into an unfamiliar world (physically or metaphorically). Their previous expertise and relationships are useless here, and they have no clear plan of action.</p>

<h3>The mentor</h3>

<p><img src="/blog/public/hero/mentor.png" alt="The mentor">
The protagonist meets someone in the wild who begins to guide their path. This is someone that they never would have met in their ordinary world, and whose advice they might have previously dismissed. They begin to gain insight about themselves and their new environment.</p>

<h3>Formulating the plan</h3>

<p><img src="/blog/public/hero/plan.png" alt="Formulating the plan">
With the mentor&#39;s help, the protagonist understands their final goal and formulates a plan to achieve it. They often embark on this quest alone, without the mentor&#39;s help, but with new knowledge gained from their time in the wild.</p>

<h3>The failure of the plan</h3>

<p><img src="/blog/public/hero/failure.png" alt="The failure of the plan">
Despite their careful planning, the plan fails. The hero was missing critical information, about their own abilities or the loyalties of their supposed allies. All appears lost, the hero is dead or near death, and the final goal appears impossible to recover.</p>

<h3>Seizing the sword</h3>

<p><img src="/blog/public/hero/seize.png" alt="Seizing the sword">
Miraculously, the hero is able to recover and continues the fight. A new plan is put into action, and the protagonist discovers new sources of strength. The enemy is defeated and goal achieved, albeit at great cost.</p>

<h3>The return</h3>

<p><img src="/blog/public/hero/ending.png" alt="The return">
The hero returns to their ordinary world reborn, and we see the consequences of their quest. The old stability is replaced with a new one, and the hero sees old relationships in a new light.</p>

<h2>Why we love the journey</h2>

<p>This story structure is so common because it has a number of nice features that help keep the audience engaged:</p>

<ul>
<li><strong>Background information is only required at the beginning.</strong> In order to follow the story and empathize with the hero, we need to be able to understand their feelings and actions. Doing this in the &quot;ordinary world&quot; is difficult, since the hero knows many things that that the audience does not know. By clearly designating that most of the story takes place in unfamiliar territory, we share the hero&#39;s confusions and assumptions, and the audience doesn&#39;t need to be constantly brought up to speed.</li>
<li><strong>There is a clear call to action.</strong> The problem faced by the hero is brought sharply to a breaking point, and there is no choice but to face it. Keeping things as they are is literally not an option.</li>
<li><strong>Help from an unlikely source.</strong> There are always &quot;typical&quot; ways of dealing with problems, such as just getting a bigger army or working harder. The mentor teaches an entirely new approach that runs counter to traditional wisdom in some way.</li>
<li><strong>The quest is near impossible.</strong> The failure of the hero&#39;s plan emphasizes that the problem being solved is extremely difficult, and that even with all their new knowledge and training they couldn&#39;t do it on their first try. When they do accomplish the task, it&#39;s only through some incredible force of will. </li>
<li><strong>The story has a impact beyond the protagonist.</strong> The stakes go beyond the selfish desires of the hero, and the ordinary world is no longer the same after the events of the story.</li>
</ul>

<h2>Takeaways for presentations</h2>

<p>If we want to make a talk into a good story, we should respect these same rules. Specifically:</p>

<ul>
<li>Background information needs to be given up front, and limited to what is strictly necessary. The audience needs to understand the current landscape as quickly as possible, and the rest of the talk should be new information that you are adding to the field.</li>
<li>Talks need to have as sense of urgency - it&#39;s not enough that something simply hasn&#39;t been studied before. Why is there an immediate, desperate need for a breakthrough in this area?</li>
<li>It&#39;s more interesting when insights come from a surprising source, such as a disconnected field or forgotten research from the past. What did you bring to the table that no one has previously tried?</li>
<li>Talks shouldn&#39;t shy away from showing how difficult a result was to achieve. For example, if presenting a complicated system you engineered, walk the audience through your thought process - what did you try first, and why did it fail? Problems that can be solved on the first try are generally not that interesting.</li>
<li>Spend time in your conclusion talking about the impact of these results on the field in general. How does this change the way people have been thinking about a problem, or enable a new line of research?</li>
</ul>

<p>Of course not all talks can use all of these elements, and this is only a starting point (since it says nothing about how to communicate the specific technical content). But we should all strive to take the audience on a journey with us during our presentation - make it an exciting one!</p>


    Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page4">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page2">Newer</a>
    
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
