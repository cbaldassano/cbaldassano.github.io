<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Connecting past and present in visual perception &middot; Rooting for the machines
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">


  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="Blog feed" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15871783-1', 'auto');
  ga('send', 'pageview');

  </script>

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@ChrisBaldassano">
  <meta name="twitter:creator" content="@ChrisBaldassano">
  
    <meta name="twitter:title" content="Connecting past and present in visual perception">
  
  
    <meta name="twitter:url" content="http://localhost:4000/2016/10/25/twonet/">
  
  
    <meta name="twitter:description" content="
There are two kinds of people in the world—those who divide everything in the world into two kinds of things and those who don’t.
Kenneth Boulding


Scientists love dividing the world into categories. Whenever we are trying to study more than 1 or 2 things at a time, our first instinct is to sort them into boxes based on their similarities, whether we&#39;re looking at animals, rocks, stars, or diseases.

There have been many proposals on how to divide up the human visual system: regions processing coarse vs. fine structure, or small objects vs. big objects, or central vs. peripheral information. In my new paper, Two distinct scene processing networks connecting vision and memory, I argue that regions activated by photographic images can be split into two different networks.











The first group of scene-processing regions (near the back of the brain) care only about the image that is currently coming in through your eyes. They are looking for visual features like walls, landmarks, and architecture that will help you determine the structure of the environment around you. But they don&#39;t try to keep track of this information over time - as soon as you move your eyes, they forget all about the last view of the world.












The second group (a bit farther forward) uses the information from the first group to build up a stable model of the world and your place in it. They care less about exactly where your eyes are pointed and more about where you are in the world, creating a 3D model of the room or landscape around you and placing you on a map of what other places are nearby. These regions are strongly linked to your long-term memory system, and show the highest activity in familiar environments.

I am very interested in this second group of regions that integrate information over time - what exactly are they keeping track of, and how do they get information in and out of long-term memory? I have a new manuscript with my collaborators at Princeton (currently working its way through the publication gaunlet) showing that these regions build abstract representations of events in movies and audio narration, and am  running a new experiment looking at how event templates we learn over our lifetimes are used to help build these event representations.
">
  


  
  <meta name="twitter:image" content="http://localhost:4000/public/visnet.png">

</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A blog about real minds, artificial minds, using artificial minds to understand real minds, and using real minds to inspire artificial minds</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://www.chrisbaldassano.com">Main</a>
    <a class="sidebar-nav-item" href="/blog/">Blog</a>


    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/blog/archive/">Blog Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    

    <a class="sidebar-nav-item" href="/blog/atom.xml">Atom feed</a>
    
  </nav>

  <div class="sidebar-item">
    <p>
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/blog" title="Home">Rooting for the machines</a>
            <small>A Blog by Chris Baldassano</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Connecting past and present in visual perception</h1>
  <span class="post-date">25 Oct 2016</span>
  <div class="message">
<p>There are two kinds of people in the world—those who divide everything in the world into two kinds of things and those who don’t.
<p>Kenneth Boulding
</div>

<p>Scientists love dividing the world into categories. Whenever we are trying to study more than 1 or 2 things at a time, our first instinct is to sort them into boxes based on their similarities, whether we&#39;re looking at animals, rocks, stars, or diseases.</p>

<p>There have been many proposals on how to divide up the human visual system: regions processing coarse vs. fine structure, or small objects vs. big objects, or <a href="/blog/2016/05/20/corner/">central vs. peripheral</a> information. In my new paper, <a href="http://eneuro.org/content/3/5/ENEURO.0178-16.2016">Two distinct scene processing networks connecting vision and memory</a>, I argue that regions activated by photographic images can be split into two different networks.
<br/>
<br/></p>

<table><thead>
<tr>
<th style="text-align: center"><img src="/blog/public/visnet.png" alt="Visual network"></th>
<th style="text-align: center"><img src="/blog/public/visnet_ex.png" alt="Visual network example"></th>
</tr>
</thead><tbody>
</tbody></table>

<p>The first group of scene-processing regions (near the back of the brain) care only about the image that is currently coming in through your eyes. They are looking for visual features like walls, landmarks, and architecture that will help you determine the structure of the environment around you. But they don&#39;t try to keep track of this information over time - as soon as you move your eyes, they forget all about the last view of the world.</p>

<p><br/>
<br/></p>

<table><thead>
<tr>
<th style="text-align: center"><img src="/blog/public/memnet.png" alt="Memory navigation network"></th>
<th style="text-align: center"><img src="/blog/public/photosynth_5fps.gif" alt="Memory network example"></th>
</tr>
</thead><tbody>
</tbody></table>

<p>The second group (a bit farther forward) uses the information from the first group to build up a stable model of the world and your place in it. They care less about exactly where your eyes are pointed and more about where you are in the world, creating a 3D model of the room or landscape around you and placing you on a map of what other places are nearby. These regions are strongly linked to your long-term memory system, and show the highest activity in familiar environments.</p>

<p>I am very interested in this second group of regions that integrate information over time - what exactly are they keeping track of, and how do they get information in and out of long-term memory? I have a new manuscript with my collaborators at Princeton (currently working its way through the publication gaunlet) showing that these regions build <a href="http://biorxiv.org/content/early/2016/10/14/081018">abstract representations of events in movies and audio narration</a>, and am  running a new experiment looking at how event templates we learn over our lifetimes are used to help build these event representations.</p>


Comments? Complaints? Contact me <a href="https://twitter.com/ChrisBaldassano">@ChrisBaldassano</a>
</div>
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
